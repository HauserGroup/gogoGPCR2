{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Peter some of these are probably unused \n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import dxpy\n",
    "import hail as hl\n",
    "from datetime import datetime\n",
    "from src.matrixtables import import_mt, interval_qc_mt, smart_split_multi_mt\n",
    "from subprocess import run\n",
    "\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.layouts import gridplot\n",
    "output_notebook()\n",
    "\n",
    "Path(\"/tmp\").resolve().mkdir(parents=True, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark and Hail\n",
    "# @Peter this can probably stay as is\n",
    "# \n",
    "\n",
    "DATABASE = \"matrix_tables\"\n",
    "REFERENCE_GENOME = 'GRCh38'\n",
    "PROJ_NAME = \"GLP2R\"\n",
    "\n",
    "LOG_FILE = (\n",
    "    Path(\"../hail_logs\", f\"{PROJ_NAME}_{datetime.now().strftime('%H%M')}.log\")\n",
    "    .resolve()\n",
    "    .__str__()\n",
    ")\n",
    "\n",
    "# Hail init\n",
    "sc = pyspark.SparkContext()\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "\n",
    "hl.init(sc=sc, default_reference=REFERENCE_GENOME, log=LOG_FILE)\n",
    "\n",
    "# Create database in DNAX\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE} LOCATION 'dnax://'\")\n",
    "mt_database = dxpy.find_one_data_object(name=DATABASE)[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Peter see here: https://dnanexus.gitbook.io/uk-biobank-rap/science-corner/whole-exome-sequencing-oqfe-protocol/generation-and-utilization-of-quality-control-set-90pct10dp-on-oqfe-data/details-on-processing-the-300k-exome-data-to-generate-the-quality-control-set \n",
    "# for a 90% DP > 10 filter that we need to add. `ukb23145_300k_OQFE.90pct10dp_qc_variants.txt` is a helper file somewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in metadata and region\n",
    "# Map genes to blocks and regions\n",
    "GENES = [\"MC4R\", \"GLP2R\"]\n",
    "MAPPING_FILE = Path(\"../../data/misc/mappings_with_blocks.tsv\").resolve()\n",
    "mapping = pd.read_csv(MAPPING_FILE, sep=\"\\t\").set_index(\"HGNC\", drop=False)\n",
    "mapping.loc[GENES,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import gVCFs as Matrix Table and filter to only GENES\n",
    "\n",
    "VCF_DIR = Path(\"/mnt/project/Bulk/Exome sequences/Population level exome OQFE variants, pVCF format - final release/\")\n",
    "FIELD_ID = 23157 # UKB field ID for latest exome data\n",
    "\n",
    "mt = import_mt(GENES, mapping, vcf_dir=VCF_DIR, vcf_version=\"v1\", field_id=FIELD_ID).key_rows_by(\n",
    "    \"locus\", \"alleles\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter to only WES target regions \n",
    "# @Peter if you want to dig up the reference for this that would be cool. Something something WES target region UKB\n",
    "\n",
    "INTERVAL_FILE=Path(\"../../data/misc/xgen_plus_spikein.b38.bed\").resolve()\n",
    "run([\"hadoop\", \"fs\", \"-put\", str(INTERVAL_FILE), \"/tmp\"])\n",
    "\n",
    "interval_table = hl.import_bed(\n",
    "        f\"/tmp/{INTERVAL_FILE.name}\",\n",
    "        reference_genome=\"GRCh38\",\n",
    "    )\n",
    "\n",
    "mt = mt.filter_rows(hl.is_defined(interval_table[mt.locus]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial checkpoint\n",
    "# @Peter might add something about why Hail likes to do checkpoint\n",
    "stage = \"INITIAL\"\n",
    "checkpoint_file = f\"/tmp/{PROJ_NAME}.{stage}.cp.mt\"\n",
    "\n",
    "mt = mt.checkpoint(checkpoint_file, overwrite=False)\n",
    "\n",
    "v, s = mt.count()\n",
    "print(f\"{v} variants and {s} samples after import and target region filter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split multi alleles \n",
    "mt = mt.filter_rows(mt.alleles.length() <= 6)\n",
    "mt = smart_split_multi_mt(mt)\n",
    "\n",
    "print(f\"{mt.count_rows()} variants with not more than 6 alleles after splitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotate with VEP and generate protein consequence\n",
    "VEP_JSON = Path(\"../../data/misc/GRCh38_VEP.json\").resolve()\n",
    "\n",
    "mt = hl.vep(mt, f\"file:{VEP_JSON}\")\n",
    "\n",
    "is_MANE = mt.aggregate_rows(\n",
    "    hl.agg.all(hl.is_defined(mt.vep.transcript_consequences.mane_select))\n",
    ")\n",
    "assert is_MANE, \"Selected transcript may not be MANE Select. Check manually.\"\n",
    "\n",
    "mt = mt.annotate_rows(\n",
    "    protCons=mt.vep.transcript_consequences.amino_acids[0].split(\"/\")[0]\n",
    "    + hl.str(mt.vep.transcript_consequences.protein_end[0])\n",
    "    + mt.vep.transcript_consequences.amino_acids[0].split(\"/\")[-1]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write final MT to DNAX\n",
    "# find database ID of newly created database using a dxpy method\n",
    "# From https://github.com/dnanexus/OpenBio/blob/master/hail_tutorial/MatrixTable_variant_annotation_with_VEP.ipynb\n",
    "MT_NAME = \"\"\n",
    "db_uri = dxpy.find_one_data_object(name=f\"{DATABASE}\", classname=\"database\")['id']\n",
    "url = f\"dnax://{db_uri}/{MT_NAME}\"\n",
    "\n",
    "\n",
    "# Note: Writing (saving/storing) the Table to the database can be computationally expensive\n",
    "# depending on the size of the annotations.\n",
    "# \n",
    "# Before this step, the Hail Table is just an object in memory. To persist it and be able to access \n",
    "# it later, the notebook needs to write it into a persistent filesystem (in this case DNAX).\n",
    "# See https://hail.is/docs/0.2/hail.Table.html#hail.Table.write for additional documentation.\n",
    "mt.write(url) # Note: output should describe size of Table (i.e. number of rows, partitions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0a0b7207fdcae6a67f17134c05e23dbf38bd1190509e28b85f351245d9a0cf9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
