{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# WGS QUALITY CONTROL & PREPARATION PIPELINE\n",
        "\n",
        "### Overview\n",
        "This notebook performs extraction, quality control, and formatting of UK Biobank Whole Genome Sequencing (WGS) data for specific genes of interest. It is optimized for the **DRAGEN 500k Release** (pVCF format).\n",
        "\n",
        "### VEP is gone\n",
        "VEP functionality has been removed from this notebook since it was brittle and heavy. Currently, .annotation and .setlist files have to be built manually until someone steps up and vibe codes an actual notebook based on the exported _variants.tsv\n",
        "\n",
        "### Workflow Logic\n",
        "1.  **Target Selection:** Loads pre-computed gene coordinates and identifies the specific VCF data blocks containing these genes.\n",
        "2.  **Data Extraction:** Imports only the relevant VCF blocks and immediately filters to the gene interval to minimize memory usage.\n",
        "3.  **Sample Quality Control:** Removes samples identified as \"poor quality\" or \"related\" from the upstream QC notebook (*01_QC_Samples*).\n",
        "4.  **Variant Quality Control:** * Filters for `FT == PASS` (or missing, as per DRAGEN standards).\n",
        "    * Removes variants with low call rates (<95%) or no alternate alleles.\n",
        "5.  **Export:**\n",
        "    * **Genotypes (.bgen):** Converts Phred-scaled likelihoods (PL) to Genotype Probabilities (GP) and exports to BGEN format for association testing (e.g., Regenie/SAIGE).\n",
        "    * **Statistics (.tsv):** Exports a clean summary of passing variants (AF, AC, Call Rate) for record-keeping.\n",
        "\n",
        "---\n",
        "\n",
        "### Required Input Files\n",
        "Before running, ensure the following files are present in the project. If you have just uploaded them, you may need to restart the Jupyter Lab VM to see them.\n",
        "\n",
        "#### 1. Gene-VCF Overlaps File (`gene_vcf_overlaps.tsv`)\n",
        "* **Description:** A mapping file that links Gene Symbols to their genomic coordinates and the specific UKB VCF block(s) that contain them.\n",
        "* **Source:** Generated by mapping the GENCODE GTF against the UKB VCF block coordinates.\n",
        "* **Columns:** `gene_name`, `chromosome`, `start`, `stop`, `overlapping_vcfs`.\n",
        "\n",
        "#### 2. Samples to Remove (`samples_to_remove.tsv`)\n",
        "* **Description:** A list of Sample IDs (EIDs) to exclude from analysis.\n",
        "* **Source:** Output of `Notebooks/WGS/01_QC_Samples.ipynb`.\n",
        "* **Content:** Includes withdrawn consent (handled by UKB/DNAx automatically, but good to verify), high-missingness samples, sex discordance, and related individuals (pruned to maximal independent set).\n",
        "\n",
        "---\n",
        "\n",
        "#### Initialization \n",
        "##### Load packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>\n",
              "        .bk-notebook-logo {\n",
              "            display: block;\n",
              "            width: 20px;\n",
              "            height: 20px;\n",
              "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
              "        }\n",
              "    </style>\n",
              "    <div>\n",
              "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
              "        <span id=\"p1001\">Loading BokehJS ...</span>\n",
              "    </div>\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n    // Clean up Bokeh references\n    if (id != null && id in Bokeh.index) {\n      Bokeh.index[id].model.document.clear();\n      delete Bokeh.index[id];\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim();\n            if (id in Bokeh.index) {\n              Bokeh.index[id].model.document.clear();\n              delete Bokeh.index[id];\n            }\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"p1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.0.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.0.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.0.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.0.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.0.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"p1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
            "application/vnd.bokehjs_load.v0+json": ""
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import dxpy\n",
        "import pyspark\n",
        "\n",
        "import hail as hl\n",
        "from pathlib import Path\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Constants\n",
        "DATABASE = \"matrix_tables\"\n",
        "REFERENCE_GENOME = \"GRCh38\"\n",
        "PROJ_NAME = \"GIPR\"\n",
        "\n",
        "Path(\"/tmp\").resolve().mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "LOG_FILE = (\n",
        "    Path(\"../hail_logs\", f\"{PROJ_NAME}_{datetime.now().strftime('%H%M')}.log\")\n",
        "    .resolve()\n",
        "    .__str__()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hail and spark configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "pip-installed Hail requires additional configuration options in Spark referring\n",
            "  to the path to the Hail Python module directory HAIL_DIR,\n",
            "  e.g. /path/to/python/site-packages/hail:\n",
            "    spark.jars=HAIL_DIR/backend/hail-all-spark.jar\n",
            "    spark.driver.extraClassPath=HAIL_DIR/backend/hail-all-spark.jar\n",
            "    spark.executor.extraClassPath=./hail-all-spark.jarRunning on Apache Spark version 3.5.2\n",
            "SparkUI available at http://ip-10-60-122-101.eu-west-2.compute.internal:8081\n",
            "Welcome to\n",
            "     __  __     <>__\n",
            "    / /_/ /__  __/ /\n",
            "   / __  / _ `/ / /\n",
            "  /_/ /_/\\_,_/_/_/   version 0.2.132-678e1f52b999\n",
            "LOGGING: writing to /opt/hail_logs/GIPR_1917.log\n",
            "2025-12-19 19:18:23.509 Hail: INFO: Reading table to impute column types\n",
            "2025-12-19 19:18:24.694 Hail: INFO: Finished type imputation\n",
            "  Loading field 'gene_id' as type str (imputed)\n",
            "  Loading field 'gene_name' as type str (imputed)\n",
            "  Loading field 'chromosome' as type str (imputed)\n",
            "  Loading field 'start' as type int32 (imputed)\n",
            "  Loading field 'stop' as type int32 (imputed)\n",
            "  Loading field 'vcf_count' as type int32 (imputed)\n",
            "  Loading field 'overlapping_vcfs' as type str (imputed)\n",
            "  Loading field 'vcf_start' as type str (imputed)\n",
            "  Loading field 'vcf_stop' as type str (imputed)\n",
            "2025-12-19 19:19:34.255 Hail: INFO: scanning VCF for sortedness...\n",
            "2025-12-19 19:20:03.851 Hail: INFO: Coerced prefix-sorted VCF, requiring additional sorting within data partitions on each query.\n",
            "2025-12-19 19:36:59.603 Hail: INFO: wrote matrix table with 16066 rows and 4810 columns in 4 partitions to /tmp/GIPR.FIRST.cp.mt\n",
            "2025-12-19 19:55:54.179 Hail: INFO: wrote matrix table with 15667 rows and 490541 columns in 4 partitions to /tmp/GIPR.FIRST.cp.mt\n",
            "2025-12-19 19:55:55.453 Hail: INFO: Reading table without type imputation\n",
            "  Loading field 'eid' as type str (not specified)\n",
            "2025-12-19 20:13:48.862 Hail: INFO: wrote matrix table with 13366 rows and 442704 columns in 4 partitions to /tmp/GIPR.SECOND.cp.mt\n",
            "2025-12-19 20:19:54.238 Hail: INFO: merging 5 files totalling 1.2M...\n",
            "2025-12-19 20:26:56.454 Hail: INFO: merging 5 files totalling 1.2M...\n",
            "2025-12-19 20:28:06.649 Hail: INFO: merging 5 files totalling 1.2M...\n",
            "2025-12-19 20:28:06.697 Hail: INFO: while writing:\n",
            "    file:/tmp/GIPR_variants.tsv\n",
            "  merge time: 47.916ms\n",
            "2025-12-19 20:43:56.335 Hail: INFO: merging 5 files totalling 1.3M...\n",
            "2025-12-19 20:43:56.353 Hail: INFO: while writing:\n",
            "    file:/tmp/GIPR_variants.tsv\n",
            "  merge time: 17.422ms\n",
            "2025-12-19 20:52:36.890 Hail: INFO: merging 5 files totalling 1.3M...\n",
            "2025-12-19 20:52:36.906 Hail: INFO: while writing:\n",
            "    file:///tmp/GIPR_variants.tsv\n",
            "  merge time: 16.407ms\n",
            "2025-12-19 21:04:32.893 Hail: INFO: merging 5 files totalling 1.3M...\n",
            "2025-12-19 21:04:32.911 Hail: INFO: while writing:\n",
            "    file:///tmp/GIPR_variants.tsv\n",
            "  merge time: 17.302ms\n"
          ]
        }
      ],
      "source": [
        "# Spark init\n",
        "sc = pyspark.SparkContext()\n",
        "spark = pyspark.sql.SparkSession(sc)\n",
        "\n",
        "# Create database in DNAX\n",
        "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE} LOCATION 'dnax://'\")\n",
        "mt_database = dxpy.find_one_data_object(name=DATABASE)[\"id\"]\n",
        "\n",
        "# Hail init\n",
        "hl.init(sc=sc, log=LOG_FILE)\n",
        "hl.default_reference(REFERENCE_GENOME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAP\n",
        "VCF_VERSION = \"v1\"\n",
        "FIELD_ID = (\n",
        "    24311  # ML-corrected DRAGEN population level WGS variants, pVCF format 500k release\n",
        ")\n",
        "VCF_DIR = Path(\n",
        "    \"DRAGEN WGS/ML-corrected DRAGEN population level WGS variants, pVCF format 500k release\"\n",
        ")\n",
        "SAMPLES_REMOVE_PATH = \"/mnt/project/TASR/Phenotypes/QC/samples_to_remove.tsv\"\n",
        "\n",
        "\n",
        "# Paths\n",
        "BULK_DIR = Path(\"/mnt/project/Bulk\")\n",
        "\n",
        "# Genes\n",
        "GENES = [\"TAS1R2\", \"GIPR\"]\n",
        "\n",
        "# Downsample for testing\n",
        "DOWNSAMPLE_FRACTION: float | None = None  # Set to 0.01 for 1% sample, 0.1 for 10%, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load\n",
        "\n",
        "#### Gene intervals and blocks\n",
        "We use the lookup table rather than build block list here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded coordinates for 2 genes: ['TAS1R2', 'GIPR']\n",
            "Identified 4 VCF files containing the genes.\n",
            "Sample path: file:///mnt/project/Bulk/DRAGEN WGS/ML-corrected DRAGEN population level WGS variants, pVCF format 500k release/chr1/ukb24311_c1_b941_v1.vcf.gz\n",
            "Identified 4 VCF files containing the genes.\n",
            "file:///mnt/project/Bulk/DRAGEN WGS/ML-corrected DRAGEN population level WGS variants, pVCF format 500k release/chr1/ukb24311_c1_b941_v1.vcf.gz\n"
          ]
        }
      ],
      "source": [
        "# --- REPLACEMENT FOR GENE INTERVALS & BLOCKS ---\n",
        "# Load the pre-computed overlap file\n",
        "OVERLAP_TSV = \"/mnt/project/WGS_Javier/WGS_QC/gene_vcf_overlaps.tsv\"  # Update path\n",
        "\n",
        "overlaps_ht = hl.import_table(f\"file://{OVERLAP_TSV}\", impute=True)  # \u2713\n",
        "\n",
        "# 1. Filter for the genes of interest\n",
        "overlaps_ht = overlaps_ht.filter(hl.literal(GENES).contains(overlaps_ht.gene_name))\n",
        "\n",
        "# 2. Extract Gene Intervals\n",
        "# We use hl.parse_locus_interval to handle \"chr:start-stop\" string format automatically\n",
        "intervals_data = overlaps_ht.select(\"chromosome\", \"start\", \"stop\").collect()\n",
        "\n",
        "gene_intervals = [\n",
        "    hl.parse_locus_interval(\n",
        "        f\"{row.chromosome}:{row.start}-{row.stop}\", reference_genome=REFERENCE_GENOME\n",
        "    )\n",
        "    for row in intervals_data\n",
        "]\n",
        "\n",
        "print(f\"Loaded coordinates for {len(gene_intervals)} genes: {GENES}\")\n",
        "\n",
        "# 3. Construct VCF Paths\n",
        "# We iterate through the rows to get the correct chromosome folder for each file.\n",
        "# Path format: {BULK_DIR}/{VCF_DIR}/{chromosome}/{filename}\n",
        "vcf_data = overlaps_ht.select(\"chromosome\", \"overlapping_vcfs\").collect()\n",
        "vcf_paths = set()\n",
        "\n",
        "# Ensure VCF_DIR is a Path object (if defined as string in constants)\n",
        "VCF_DIR_PATH = BULK_DIR / Path(VCF_DIR)\n",
        "\n",
        "for row in vcf_data:\n",
        "    if not row.overlapping_vcfs:\n",
        "        continue\n",
        "\n",
        "    # Split \"file1.vcf,file2.vcf\" -> [\"file1.vcf\", \"file2.vcf\"]\n",
        "    files = row.overlapping_vcfs.split(\",\")\n",
        "\n",
        "    for f in files:\n",
        "        f = f.strip()\n",
        "        if not f:\n",
        "            continue\n",
        "\n",
        "        # Use the 'chromosome' column from the TSV (e.g., \"chr19\") for the folder\n",
        "        full_path = f\"file://{VCF_DIR_PATH}/{row.chromosome}/{f}\"\n",
        "        vcf_paths.add(full_path)\n",
        "\n",
        "vcf_files = list(vcf_paths)\n",
        "print(f\"Identified {len(vcf_files)} VCF files containing the genes.\")\n",
        "print(\"Sample path:\", vcf_files[0] if vcf_files else \"None\")\n",
        "print(f\"Identified {len(vcf_files)} VCF files containing the genes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if Path(\n",
        "    SAMPLES_REMOVE_PATH.replace(\"file://\", \"\")\n",
        ").exists() or SAMPLES_REMOVE_PATH.startswith(\"dnax://\"):\n",
        "    samples_to_remove = hl.import_table(f\"file://{SAMPLES_REMOVE_PATH}\", key=\"eid\")\n",
        "else:\n",
        "    samples_to_remove = None\n",
        "    print(\n",
        "        f\"WARNING: QC file not found at {SAMPLES_REMOVE_PATH}. Skipping sample removal.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Global fields:\n",
            "    None\n",
            "----------------------------------------\n",
            "Column fields:\n",
            "    's': str\n",
            "----------------------------------------\n",
            "Row fields:\n",
            "    'locus': locus<GRCh38>\n",
            "    'alleles': array<str>\n",
            "    'rsid': str\n",
            "    'qual': float64\n",
            "    'filters': set<str>\n",
            "    'info': struct {\n",
            "        AC: array<int32>, \n",
            "        AN: int32, \n",
            "        NS: int32, \n",
            "        NS_GT: int32, \n",
            "        NS_NOGT: int32, \n",
            "        NS_NODATA: int32, \n",
            "        IC: float64, \n",
            "        HWE: array<float64>, \n",
            "        ExcHet: array<float64>, \n",
            "        HWE_CHISQ: float64, \n",
            "        AF: array<float64>\n",
            "    }\n",
            "----------------------------------------\n",
            "Entry fields:\n",
            "    'GT': call\n",
            "    'GQ': int32\n",
            "    'LAD': array<int32>\n",
            "    'FT': str\n",
            "    'LPL': array<int32>\n",
            "    'LAA': array<str>\n",
            "    'LAF': array<float64>\n",
            "    'QL': float64\n",
            "----------------------------------------\n",
            "Column key: ['s']\n",
            "Row key: ['locus', 'alleles']\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# --- IMPORT VCFS ---\n",
        "mt = hl.import_vcf(\n",
        "    vcf_files,\n",
        "    drop_samples=False,\n",
        "    reference_genome=REFERENCE_GENOME,\n",
        "    array_elements_required=True,\n",
        "    force_bgz=True,\n",
        ")\n",
        "\n",
        "# Avoid write when checkpoint\n",
        "if samples_to_remove is not None:\n",
        "    mt = mt.anti_join_cols(samples_to_remove)\n",
        "\n",
        "\n",
        "# Also, watch out\n",
        "if DOWNSAMPLE_FRACTION is not None:\n",
        "    print(f\"\u26a0\ufe0f  DOWNSAMPLING to {DOWNSAMPLE_FRACTION*100}% of samples for testing\")\n",
        "    mt = mt.sample_cols(p=DOWNSAMPLE_FRACTION, seed=42)\n",
        "\n",
        "# Filter MT to exact gene boundaries (removes flanking regions in the 20kb block)\n",
        "mt = hl.filter_intervals(mt, gene_intervals)\n",
        "\n",
        "# Godlike filtering\n",
        "mt = mt.filter_rows(hl.len(mt.filters) == 0)\n",
        "\n",
        "# 2. Filter Entries (Genotype Level)\n",
        "# ML-corrected data is cleaner, but we still enforce PASS and basic depth/quality if available\n",
        "# TODO: Find some actual documentation here\n",
        "# Minimal load for the |\n",
        "\n",
        "mt = mt.filter_entries(hl.is_missing(mt.FT) | (mt.FT == \"PASS\"))\n",
        "\n",
        "\n",
        "mt.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Checkpoint MT\n",
        "Checkpoint after coarse filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Global fields:\n",
            "    None\n",
            "----------------------------------------\n",
            "Column fields:\n",
            "    's': str\n",
            "----------------------------------------\n",
            "Row fields:\n",
            "    'locus': locus<GRCh38>\n",
            "    'alleles': array<str>\n",
            "    'rsid': str\n",
            "    'qual': float64\n",
            "    'filters': set<str>\n",
            "    'info': struct {\n",
            "        AC: array<int32>, \n",
            "        AN: int32, \n",
            "        NS: int32, \n",
            "        NS_GT: int32, \n",
            "        NS_NOGT: int32, \n",
            "        NS_NODATA: int32, \n",
            "        IC: float64, \n",
            "        HWE: array<float64>, \n",
            "        ExcHet: array<float64>, \n",
            "        HWE_CHISQ: float64, \n",
            "        AF: array<float64>\n",
            "    }\n",
            "----------------------------------------\n",
            "Entry fields:\n",
            "    'GT': call\n",
            "    'GQ': int32\n",
            "    'LAD': array<int32>\n",
            "    'FT': str\n",
            "    'LPL': array<int32>\n",
            "    'LAA': array<str>\n",
            "    'LAF': array<float64>\n",
            "    'QL': float64\n",
            "----------------------------------------\n",
            "Column key: ['s']\n",
            "Row key: ['locus', 'alleles']\n",
            "----------------------------------------\n",
            "Pre-QC count: 15667 variants, 490541 samples\n"
          ]
        }
      ],
      "source": [
        "# First checkpoint\n",
        "\n",
        "stage = \"FIRST\"\n",
        "checkpoint_file = f\"/tmp/{PROJ_NAME}.{stage}.cp.mt\"\n",
        "\n",
        "mt = mt.checkpoint(checkpoint_file, overwrite=True)\n",
        "# mt = hl.read_matrix_table(checkpoint_file)\n",
        "\n",
        "mt.describe()\n",
        "print(f\"Pre-QC count: {mt.count_rows()} variants, {mt.count_cols()} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Quality control filtering\n",
        "Remove samples from 01_QC_Samples\n",
        "\n",
        "Filter to FT (empty? Documentation says PASS? Who knows?)\n",
        "\n",
        "A lot of the entry fields are populated, so check if ever adding more filters. GQ and LPL are empty for example. \n",
        "\n",
        "GT is as usual the one we want. \n",
        "\n",
        "variant_qc is run last to save a little compute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Sample QC...\n",
            "Running Variant QC...\n"
          ]
        }
      ],
      "source": [
        "# --- QC & FILTERING ---\n",
        "\n",
        "# 4. Filter Samples (Columns)\n",
        "# Remove samples with low call rate (< 95%)\n",
        "mt = hl.sample_qc(mt)\n",
        "mt = mt.filter_cols(mt.sample_qc.call_rate > 0.95)\n",
        "\n",
        "\n",
        "# 3. Run Variant QC\n",
        "mt = hl.variant_qc(mt)  # needs this for later too\n",
        "mt = mt.filter_rows(\n",
        "    (mt.variant_qc.n_non_ref > 0)  # Must have at least one Alt allele\n",
        "    & (mt.variant_qc.call_rate > 0.95)  # High call rate\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Post-QC count: 13366 variants, 442704 samples\n"
          ]
        }
      ],
      "source": [
        "# Second checkpoint\n",
        "# Hail still likes checkpointing\n",
        "# TODO: consider re-instating but likely not needed here\n",
        "\n",
        "# stage = \"SECOND\"\n",
        "# checkpoint_file = f\"/tmp/{PROJ_NAME}.{stage}.cp.mt\"\n",
        "\n",
        "# mt = mt.checkpoint(checkpoint_file, overwrite=True)\n",
        "# print(f\"Post-QC count: {mt.count_rows()} variants, {mt.count_cols()} samples\")\n",
        "# mt.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exports\n",
        "\n",
        "Again, Regenie checks bgen GP but wants GT\n",
        "\n",
        "Removed all VEP logic from here. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exporting BGEN to /tmp/GIPR.bgen ... (takes a little while)\n",
            "Exporting Stats to /tmp/GIPR_variants.tsv ...\n"
          ]
        }
      ],
      "source": [
        "# --- EXPORTS (Cleaned Up) ---\n",
        "\n",
        "# Define Paths\n",
        "LOCAL_TMP = \"/tmp\"  # Local container storage\n",
        "HDFS_TMP = \"file:///tmp\"  # Explicitly pointing to local FS for Hail\n",
        "DX_OUTPUT_DIR = \"/TASR/Genotypes\"\n",
        "BASE_NAME = f\"{PROJ_NAME}\"\n",
        "\n",
        "# 1. Prepare Annotations (GP & VarID)\n",
        "# Fallback to Hard Calls (since we know PL/LPL are likely missing/empty)\n",
        "GPs = hl.literal([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]])\n",
        "mt = mt.annotate_entries(GP=GPs[mt.GT.n_alt_alleles()])\n",
        "\n",
        "mt = mt.annotate_rows(\n",
        "    varid=hl.delimit(\n",
        "        [mt.locus.contig, hl.str(mt.locus.position), mt.alleles[0], mt.alleles[1]], \":\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# 2. Export BGEN (To Local /tmp via \"file://\")\n",
        "# \"parallel=None\" ensures we get a single .bgen file, not a directory of shards.\n",
        "# We use \"file://\" prefix to force Hail to write to the local node's /tmp,\n",
        "# making it instantly accessible for dx upload without hadoop fs -get.\n",
        "print(f\"Exporting BGEN to {LOCAL_TMP}/{BASE_NAME}.bgen ... (takes a little while)\")\n",
        "\n",
        "hl.export_bgen(\n",
        "    mt=mt,\n",
        "    output=f\"{HDFS_TMP}/{BASE_NAME}\",  # file:///tmp/GIPR\n",
        "    gp=mt.GP,\n",
        "    varid=mt.varid,\n",
        "    rsid=mt.varid,\n",
        "    parallel=None,\n",
        ")\n",
        "\n",
        "# 3. Export Variant Stats TSV\n",
        "rows_ht = mt.rows()\n",
        "\n",
        "export_ht = rows_ht.select(\n",
        "    chromosome=rows_ht.locus.contig,\n",
        "    position=rows_ht.locus.position,\n",
        "    ref=rows_ht.alleles[0],\n",
        "    alt=rows_ht.alleles[1],\n",
        "    rsid=rows_ht.rsid,\n",
        "    call_rate=rows_ht.variant_qc.call_rate,\n",
        "    AC=rows_ht.variant_qc.AC[1],\n",
        "    AF=rows_ht.variant_qc.AF[1],\n",
        "    AN=rows_ht.variant_qc.AN,\n",
        "    N_HOM=rows_ht.variant_qc.homozygote_count[1],\n",
        "    N_HET=rows_ht.variant_qc.n_het,\n",
        "    N_NC=rows_ht.variant_qc.n_not_called,\n",
        ")\n",
        "\n",
        "tsv_name = f\"{BASE_NAME}_variants.tsv\"\n",
        "print(f\"Exporting Stats to {LOCAL_TMP}/{tsv_name} ...\")\n",
        "export_ht.export(f\"{HDFS_TMP}/{tsv_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# We now upload directly from /tmp where Hail wrote the files\n",
        "!dx upload {LOCAL_TMP}/{BASE_NAME}.bgen {LOCAL_TMP}/{BASE_NAME}.sample {LOCAL_TMP}/{tsv_name} --path {DX_OUTPUT_DIR}"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
