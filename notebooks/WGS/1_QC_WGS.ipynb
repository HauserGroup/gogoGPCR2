{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRIPT TO PERFORM QUALITY CONTROL ON WHOLE-GENOME SEQUENCING DATA\n",
    "\n",
    "In order to run, there has to be several files in the project folder:\n",
    "- GENCODE GTF: Run Scripts/WGS/01_get_gencode_annotation.sh. Obtain from: https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_46/gencode.v46.annotation.gtf.gz (Check for newer versions).\n",
    "\n",
    "Once completed, a new Jupyter Notebook should be initialized so we can access this file. Or unmount and mount again the project (https://community.ukbiobank.ac.uk/hc/en-gb/community/posts/16019592366365-It-seems-that-the-recently-dx-uploaded-files-does-not-show-up-on-mnt-project-until-I-re-start-the-whole-Jupyter-Lab-VM)\n",
    "\n",
    "\n",
    "- PVCF BLOCKS: Run Notebooks/WGS/DragenBlockProcessing.ipynb. Obtain from: https://biobank.ndph.ox.ac.uk/ukb/ukb/auxdata/dragen_pvcf_coordinates.zip \n",
    "It needs parsing, but in data/misc it is already parsed.\n",
    "\n",
    "- Samples to remove file: Run Notebooks/WGS/01_QC_Samples.ipynb\n",
    "\n",
    "#### Initialization \n",
    "##### Load packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dxpy\n",
    "import pyspark\n",
    "\n",
    "import hail as hl\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from src.matrixtables import smart_split_multi_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATABASE = \"matrix_tables\"\n",
    "REFERENCE_GENOME = \"GRCh38\"\n",
    "PROJ_NAME = \"GIPR\"\n",
    "\n",
    "Path(\"/tmp\").resolve().mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOG_FILE = (\n",
    "    Path(\"../hail_logs\", f\"{PROJ_NAME}_{datetime.now().strftime('%H%M')}.log\")\n",
    "    .resolve()\n",
    "    .__str__()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hail and spark configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark init\n",
    "sc = pyspark.SparkContext()\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "\n",
    "# Create database in DNAX\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE} LOCATION 'dnax://'\")\n",
    "mt_database = dxpy.find_one_data_object(name=DATABASE)[\"id\"]\n",
    "\n",
    "# Hail init\n",
    "hl.init(sc=sc, default_reference=REFERENCE_GENOME, log=LOG_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAP\n",
    "VCF_VERSION = \"v1\"\n",
    "FIELD_ID = 24310 # DRAGEN population level WGS variants, pVCF format 500k release\n",
    "\n",
    "# Paths\n",
    "BULK_DIR = Path(\"/mnt/project/Bulk\")\n",
    "\n",
    "# Genes\n",
    "GENES = [\"GIPR\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality control\n",
    "\n",
    "#### Gene intervals and blocks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gene intervals\n",
    "gene_interval = hl.experimental.get_gene_intervals(\n",
    "    gene_symbols=GENES,\n",
    "    reference_genome=\"GRCh38\",\n",
    "    gtf_file=\"file:///mnt/project/WGS_Javier/WGS_QC/gencode.v46.annotation.gtf\",\n",
    ")\n",
    "gene_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DRAGEN pVCF blocks\n",
    "blocks = hl.import_table(\"file:///mnt/project/WGS_Javier/WGS_QC/dragen_pvcf_blocks.tsv\", no_header=False)\n",
    "blocks = blocks.annotate(Chromosome=blocks.Chromosome.replace(\"23\", \"X\").replace(\"24\", \"Y\"))\n",
    "blocks = blocks.annotate(region=hl.str(\"\").join([hl.str(\"chr\"), blocks.Chromosome]))\n",
    "blocks = blocks.annotate(\n",
    "    interval=hl.locus_interval(\n",
    "        blocks.region,\n",
    "        hl.int32(blocks.Starting_Position),\n",
    "        hl.int32(blocks.Ending_Position),\n",
    "        reference_genome=\"GRCh38\",\n",
    "    )\n",
    ").key_by(\"interval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get blocks for given genes\n",
    "gb = blocks.filter(hl.any(lambda inter: blocks.interval.overlaps(inter), gene_interval))\n",
    "gb.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import vcf files of specific blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VCF_DIR = Path(\"DRAGEN WGS/DRAGEN population level WGS variants, pVCF format 500k release\")\n",
    "\n",
    "vcf_files = [\n",
    "    f\"file://{BULK_DIR / VCF_DIR}/{chromosome}/ukb{FIELD_ID}_c{chromosome.replace('chr', '')}_b{block}_{VCF_VERSION}.vcf.gz\"\n",
    "    for block, chromosome in zip(gb.Block.collect(), gb.region.collect())\n",
    "]\n",
    "\n",
    "mt = hl.import_vcf(\n",
    "    vcf_files,\n",
    "    drop_samples=False,\n",
    "    reference_genome=\"GRCh38\",\n",
    "    array_elements_required=False,\n",
    "    force_bgz=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only genes of interest\n",
    "mt = hl.filter_intervals(mt, gene_interval)\n",
    "print(f\"{mt.count_rows()} variants after gene filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First checkpoint\n",
    "stage = \"FIRST\"\n",
    "checkpoint_file = f\"/tmp/{PROJ_NAME}.{stage}.cp.mt\"\n",
    "\n",
    "mt = mt.checkpoint(checkpoint_file, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-allele filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove variants with 6 or more alleles\n",
    "mt = mt.filter_rows(mt.alleles.length() <= 6)\n",
    "print(f\"{mt.count_rows()} variants with not more than 6 alleles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split multi-allele variants into single ones\n",
    "mt = smart_split_multi_mt(mt)\n",
    "print(f\"{mt.count_rows()} variants after multi-allele splitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality control filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = mt.filter_entries(mt.FT == \"PASS\")\n",
    "\n",
    "# Then, filter variants where there is at least one non-missing genotype\n",
    "mt = mt.filter_rows(hl.agg.any(hl.is_defined(mt.GT)))\n",
    "print(f\"{mt.count_rows()} variants after entries quality filtering\")\n",
    "print(f\"{mt.count_cols()} samples after entries quality filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics about the number and fraction of filtered entries.\n",
    "mt = hl.MatrixTable.compute_entry_filter_stats(mt, row_field='entry_stats_row', col_field='entry_stats_col')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_fraction_threshold = 0.95\n",
    "\n",
    "# Filter variants where at least 95% of genotypes are unfiltered\n",
    "mt = mt.filter_rows(\n",
    "    (1 - mt.entry_stats_row.fraction_filtered) > row_fraction_threshold\n",
    ")\n",
    "\n",
    "print(f\"{mt.count_rows()} variants after fraction-based filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_fraction_threshold = 0.95\n",
    "\n",
    "# Filter samples where at least 95% of variants are unfiltered\n",
    "mt = mt.filter_cols(\n",
    "    (1 - mt.entry_stats_col.fraction_filtered) > col_fraction_threshold\n",
    ")\n",
    "\n",
    "print(f\"{mt.count_cols()} samples after fraction-based filtering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove samples from 01_QC_Samples.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_to_remove = hl.import_table(\"file:///mnt/project/WGS_Javier/Data/Input_regenie/samples_to_remove.tsv\", key=\"eid\")\n",
    "\n",
    "mt = mt.anti_join_cols(samples_to_remove)\n",
    "\n",
    "# Filter rows (variants) where any sample information is still present\n",
    "mt = mt.filter_rows(hl.agg.any(mt.GT.n_alt_alleles() > 0))\n",
    "\n",
    "print(f\"Samples remaining after removing samples from QC samples: {mt.count_cols()} \")\n",
    "print(f\"Variants remaining after removing samples from QC samples: {mt.count_rows()} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second checkpoint\n",
    "stage = \"SECOND\"\n",
    "checkpoint_file = f\"/tmp/{PROJ_NAME}.{stage}.cp.mt\"\n",
    "\n",
    "mt = mt.checkpoint(checkpoint_file, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variant Effect Predictor (VEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VEP_JSON = Path(\"GRCh38_VEP.json\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = hl.vep(mt, f\"file:{VEP_JSON}\", block_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_MANE = mt.aggregate_rows(\n",
    "    hl.agg.all(hl.is_defined(mt.vep.transcript_consequences.mane_select))\n",
    ")\n",
    "assert is_MANE, \"Selected transcript may not be MANE Select. Check manually.\"\n",
    "\n",
    "mt = mt.annotate_rows(\n",
    "    protCons=mt.vep.transcript_consequences.amino_acids[0].split(\"/\")[0]\n",
    "    + hl.str(mt.vep.transcript_consequences.protein_end[0])\n",
    "    + mt.vep.transcript_consequences.amino_acids[0].split(\"/\")[-1],\n",
    "    varid=hl.variant_str(mt.locus, mt.alleles)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third checkpoint\n",
    "stage = \"THIRD\"\n",
    "checkpoint_file = f\"/tmp/{PROJ_NAME}.{stage}.cp.mt\"\n",
    "\n",
    "mt = mt.checkpoint(checkpoint_file, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = hl.variant_qc(mt)\n",
    "mt = mt.filter_rows(mt.variant_qc.n_non_ref > 0)\n",
    "mt = mt.filter_rows(mt.variant_qc.gq_stats.mean >= 20)\n",
    "mt = mt.filter_rows(mt.variant_qc.call_rate >= 0.95)\n",
    "mt = mt.filter_rows(mt.vep.most_severe_consequence != \"intron_variant\")\n",
    "mt = mt.filter_rows(mt.vep.most_severe_consequence != \"downstream_gene_variant\")\n",
    "mt = mt.filter_rows(mt.vep.most_severe_consequence != \"upstream_gene_variant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{mt.count_rows()} variants after quality filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forth checkpoint\n",
    "stage = \"FORTH\"\n",
    "checkpoint_file = f\"/tmp/{PROJ_NAME}.{stage}.cp.mt\"\n",
    "\n",
    "mt = mt.checkpoint(checkpoint_file, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt = mt.rows()\n",
    "qt = qt.explode(qt.vep.transcript_consequences)\n",
    "\n",
    "qt = qt.select(\n",
    "    qt.varid,\n",
    "    qt.protCons,\n",
    "    qt.vep.most_severe_consequence,\n",
    "    qt.vep.transcript_consequences.protein_end,\n",
    "    qt.vep.transcript_consequences.protein_start,\n",
    "    qt.vep.transcript_consequences.amino_acids,\n",
    "    qt.vep.transcript_consequences.gene_id,\n",
    "    qt.vep.transcript_consequences.transcript_id,\n",
    "    **qt.variant_qc.flatten(),\n",
    ")\n",
    "\n",
    "qt = qt.annotate(AC=qt.AC[1], AF=qt.AF[1], homozygote_count=qt.homozygote_count[1])\n",
    "qt = qt.key_by().drop(\"locus\", \"alleles\")\n",
    "\n",
    "qt.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by each distinct 'most_severe_consequence' and count the number of occurrences\n",
    "consequence_counts = qt.aggregate(\n",
    "    hl.agg.group_by(qt.most_severe_consequence, hl.agg.count())\n",
    ")\n",
    "\n",
    "print(consequence_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt.export(\"/tmp/variant_qc.tsv\")\n",
    "!hadoop fs -getmerge /tmp/variant_qc.tsv ../variant_qc.tsv\n",
    "!dx upload ../variant_qc.tsv --path /WGS_Javier/WGS_QC/Output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BGEN file\n",
    "BGEN_FILE = \"/tmp/GIPR\"\n",
    "GPs = hl.literal([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]])\n",
    "\n",
    "mt = mt.annotate_entries(GP=GPs[mt.GT.n_alt_alleles()])\n",
    "\n",
    "hl.export_bgen(\n",
    "    mt=mt, varid=mt.varid, rsid=mt.varid, gp=mt.GP, output=\"file:\" + BGEN_FILE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANNOTATIONS file\n",
    "ANNOTATIONS_FILE = \"/tmp/GIPR.annotations\"\n",
    "\n",
    "annotations = (\n",
    "    mt.select_rows(\n",
    "        varid=mt.varid,\n",
    "        gene=mt.vep.transcript_consequences.gene_symbol[0],\n",
    "        annotation=hl.if_else(\n",
    "            # Check if 'protCons' is missing, if so, use \"most_severe_consequence\"\n",
    "            hl.is_missing(mt.protCons),  \n",
    "            mt.vep.most_severe_consequence,  \n",
    "            mt.protCons \n",
    "        )\n",
    "    )\n",
    "    .rows()\n",
    "    .key_by(\"varid\")\n",
    "    .drop(\"locus\")\n",
    "    .drop(\"alleles\")\n",
    ")\n",
    "\n",
    "annotations.export(\"file:\" + ANNOTATIONS_FILE, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETLIST file\n",
    "SETLIST_FILE = \"/tmp/GIPR.setlist\"\n",
    "position = mt.aggregate_rows(hl.agg.min(mt.locus.position))\n",
    "names = mt.varid.collect()\n",
    "names_str = \",\".join(names)\n",
    "\n",
    "line = f\"{mt.vep.transcript_consequences.gene_symbol.collect()[0]}\\t{mt.locus.contig.collect()[0]}\\t{position}\\t{names_str}\"\n",
    "\n",
    "with open(SETLIST_FILE, \"w\") as f:\n",
    "    f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgen_file = BGEN_FILE + \".bgen\"\n",
    "sample_file = BGEN_FILE + \".sample\"\n",
    "\n",
    "!dx upload $bgen_file $sample_file $ANNOTATIONS_FILE $SETLIST_FILE --path /WGS_Javier/WGS_QC/Output/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
