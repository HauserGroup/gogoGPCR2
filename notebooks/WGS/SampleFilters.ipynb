{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4366a523-07d4-487e-9595-a8949aaf127d",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import dxdata\n",
        "import dxpy\n",
        "import hail as hl\n",
        "import pyspark\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "from src.fields import fields_for_id\n",
        "\n",
        "from packaging import version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8b7c5057-4e18-43e3-b30f-40d1e8bbc23a",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "SLF4J: Class path contains multiple SLF4J bindings.\n",
            "SLF4J: Found binding in [jar:file:/cluster/dnax/jars/dnanexus-api-0.1.0-SNAPSHOT-jar-with-dependencies.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: Found binding in [jar:file:/cluster/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
            "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-09-13 08:36:00.385 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "2023-09-13 08:36:01.462 WARN  Utils:69 - Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 43000. Attempting port 43001.\n",
            "2023-09-13 08:36:01.720 WARN  MetricsReporter:84 - No metrics configured for reporting\n",
            "2023-09-13 08:36:01.722 WARN  LineProtoUsageReporter:48 - Telegraf configurations: url [metrics.push.telegraf.hostport], user [metrics.push.telegraf.user] or password [metrics.push.telegraf.password] missing.\n",
            "2023-09-13 08:36:01.722 WARN  MetricsReporter:117 - metrics.scraping.httpserver.port\n"
          ]
        }
      ],
      "source": [
        "# Initialise Spark\n",
        "\n",
        "sc = pyspark.SparkContext()\n",
        "spark = SparkSession(sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "28b5e832-faba-408f-924c-a4b5d46e56f4",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "DATABASE = \"matrix_tables\"\n",
        "REFERENCE_GENOME = \"GRCh38\"\n",
        "\n",
        "LOG_FILE = (\n",
        "    Path(\"../hail_logs\", f\"hail_{datetime.now().strftime('%H%M')}.log\")\n",
        "    .resolve()\n",
        "    .__str__()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1918ea4a-bad7-4468-9d4f-da96f3b6c723",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "pip-installed Hail requires additional configuration options in Spark referring\n",
            "  to the path to the Hail Python module directory HAIL_DIR,\n",
            "  e.g. /path/to/python/site-packages/hail:\n",
            "    spark.jars=HAIL_DIR/backend/hail-all-spark.jar\n",
            "    spark.driver.extraClassPath=HAIL_DIR/backend/hail-all-spark.jar\n",
            "    spark.executor.extraClassPath=./hail-all-spark.jar"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "log4j: Parsing for [root] with value=[INFO, logfile].\n",
            "log4j: Level token is [INFO].\n",
            "log4j: Category root set to INFO\n",
            "log4j: Parsing appender named \"logfile\".\n",
            "log4j: Parsing layout options for \"logfile\".\n",
            "log4j: Setting property [conversionPattern] to [%d{yyyy-MM-dd HH:mm:ss.SSS} %c{1}: %p: %m%n].\n",
            "log4j: End of parsing for \"logfile\".\n",
            "log4j: Setting property [append] to [false].\n",
            "log4j: Setting property [threshold] to [INFO].\n",
            "log4j: Setting property [file] to [/opt/notebooks/gogoGPCR2/hail_logs/DRD2_0836.log].\n",
            "log4j: setFile called: /opt/notebooks/gogoGPCR2/hail_logs/DRD2_0836.log, false\n",
            "log4j: setFile ended\n",
            "log4j: Parsed \"logfile\" options.\n",
            "log4j: Parsing for [Hail] with value=[INFO, HailSocketAppender].\n",
            "log4j: Level token is [INFO].\n",
            "log4j: Category Hail set to INFO\n",
            "log4j: Parsing appender named \"HailSocketAppender\".\n",
            "log4j: Parsed \"HailSocketAppender\" options.\n",
            "log4j: Handling log4j.additivity.Hail=[null]\n",
            "log4j: Finished configuring.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running on Apache Spark version 3.2.3\n",
            "SparkUI available at http://ip-10-60-22-64.eu-west-2.compute.internal:8081\n",
            "Welcome to\n",
            "     __  __     <>__\n",
            "    / /_/ /__  __/ /\n",
            "   / __  / _ `/ / /\n",
            "  /_/ /_/\\_,_/_/_/   version 0.2.116-cd64e0876c94\n",
            "LOGGING: writing to /opt/notebooks/gogoGPCR2/hail_logs/DRD2_0836.log\n"
          ]
        }
      ],
      "source": [
        "# Hail init\n",
        "hl.init(sc=sc, default_reference=REFERENCE_GENOME, log=LOG_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "487d857f-9c15-41fe-b799-00b418258f60",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "dispensed_database_name = dxpy.find_one_data_object(\n",
        "    classname=\"database\", name=\"app*\", folder=\"/\", name_mode=\"glob\", describe=True\n",
        ")[\"describe\"][\"name\"]\n",
        "dispensed_dataset_id = dxpy.find_one_data_object(\n",
        "    typename=\"Dataset\", name=\"app*.dataset\", folder=\"/\", name_mode=\"glob\"\n",
        ")[\"id\"]\n",
        "\n",
        "dataset = dxdata.load_dataset(id=dispensed_dataset_id)  # type: ignore\n",
        "participant = dataset[\"participant\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b33473f8-5a72-4edf-be1c-c6d0ab8b073b",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "fields = [\"22027\", \"22019\", \"22021\", \"21000\"]\n",
        "\n",
        "field_names = [fields_for_id(i, participant) for i in fields]\n",
        "field_names = [\"eid\"] + [field.name for fields in field_names for field in fields]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "96cd863a-23c8-477f-956a-2c13a09f33ba",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 11:=============================================>            (7 + 2) / 9]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples to be filtered: 4713\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Use hard filters\n",
        "\n",
        "df = participant.retrieve_fields(\n",
        "    names=field_names,\n",
        "    engine=dxdata.connect(),\n",
        "    coding_values=\"replace\",  # type: ignore\n",
        ")\n",
        "\n",
        "df = df.filter(\n",
        "    (~df.p22027.isNull())\n",
        "    | (~df.p22019.isNull())\n",
        "    | (df.p22021 == \"Participant excluded from kinship inference process\")\n",
        "    | (df.p22021 == \"Ten or more third-degree relatives identified\")\n",
        "    | (df.p21000_i0 == \"White and Black Caribbean\")\n",
        "    | (df.p21000_i0 == \"White and Black African\")\n",
        "    | (df.p21000_i0 == \"White and Asian\")\n",
        "    | (df.p21000_i0 == \"Any other mixed background\")\n",
        ")\n",
        "filtered_samples_to_remove = hl.Table.from_spark(df.select(\"eid\")).key_by(\"eid\")\n",
        "\n",
        "print(f\"Samples to be filtered: {filtered_samples_to_remove.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "755362df-ad24-4161-86f6-081c15cdd655",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-13 08:36:39.239 Hail: INFO: Reading table without type imputation       \n",
            "  Loading field '' as type str (not specified)\n",
            "  Loading field 'PC_UKBB.eid' as type str (not specified)\n",
            "  Loading field 'group' as type str (not specified)\n",
            "[Stage 2:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ancestry to remove: 30171\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Use ancestry filter from Privet et al.\n",
        "ANCESTRY_FILE = \"file:///mnt/project/gogoGPCR2/ancestry.csv\"\n",
        "\n",
        "anc = hl.import_table(ANCESTRY_FILE, delimiter=\",\", quote='\"')\n",
        "anc = anc.key_by(eid=anc[\"PC_UKBB.eid\"])\n",
        "ancestry_to_remove = anc.filter(anc.group != \"United Kingdom\")\n",
        "print(f\"Ancestry to remove: {ancestry_to_remove.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f10af36f-dc12-4357-8530-7921987bb661",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-13 08:42:50.839 Hail: INFO: Reading table without type imputation       \n",
            "  Loading field 'f0' as type str (not specified)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Withdrawn samples to remove: 148\n"
          ]
        }
      ],
      "source": [
        "# Withdrawn samples to remove\n",
        "df_withdrawn = df.filter(~df.eid.startswith(\"w\"))\n",
        "\n",
        "withdrawn_to_remove = hl.Table.from_spark(df_filtered.select(\"eid\")).key_by(\"eid\")\n",
        "print(f\"Withdrawn samples to remove: {withdrawn_to_remove.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7f4d6510-6c36-43b8-9fd6-094d6b29390d",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-13 08:50:08.110 Hail: INFO: Reading table to impute column types        \n",
            "2023-09-13 08:50:09.220 Hail: INFO: Finished type imputation\n",
            "  Loading field 'ID1' as type str (user-supplied type)\n",
            "  Loading field 'ID2' as type str (user-supplied type)\n",
            "  Loading field 'HetHet' as type float64 (imputed)\n",
            "  Loading field 'IBS0' as type float64 (imputed)\n",
            "  Loading field 'Kinship' as type float64 (imputed)\n",
            "2023-09-13 08:50:14.794 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:50:17.096 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:50:20.314 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:50:22.316 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:50:24.999 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:50:26.448 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:50:33.030 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:50:34.215 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:50:36.471 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:50:37.062 Hail: INFO: Coerced sorted dataset\n",
            "2023-09-13 08:50:38.547 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:50:39.110 Hail: INFO: Coerced sorted dataset\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Related samples not already in filter and high kinship coefficient: 27649\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Affective cases: 33439\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-13 08:50:49.931 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:50:51.825 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:50:54.413 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:50:56.368 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:50:58.835 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:51:00.477 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:51:02.559 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:51:03.707 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:51:05.405 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:51:05.953 Hail: INFO: Coerced sorted dataset\n",
            "2023-09-13 08:51:07.469 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:51:08.107 Hail: INFO: Coerced sorted dataset\n",
            "2023-09-13 08:51:09.856 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:51:10.911 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:51:13.712 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:51:14.719 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:51:19.258 Hail: INFO: wrote table with 27649 rows in 1 partition to /tmp/jhLoQyVXJd3udmUQpXNpDh\n",
            "2023-09-13 08:51:27.216 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "[Stage 153:>                                                        (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples to remove to create independent set: 24848\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Remove related individuals\n",
        "RAW_REL_FILE = Path(\"/mnt/project/Bulk/Genotype Results/Genotype calls/ukb_rel.dat\")\n",
        "MAX_KINSHIP = 0.176  # 2nd degree relatives TODO: Check this value\n",
        "\n",
        "\n",
        "rel = hl.import_table(\n",
        "    f\"file://{RAW_REL_FILE}\",\n",
        "    delimiter=\" \",\n",
        "    impute=True,\n",
        "    types={\"ID1\": \"str\", \"ID2\": \"str\"},\n",
        ")\n",
        "\n",
        "rel = rel.filter(\n",
        "    hl.is_defined(filtered_samples_to_remove[rel.ID1])\n",
        "    | hl.is_defined(filtered_samples_to_remove[rel.ID2])\n",
        "    | hl.is_defined(ancestry_to_remove[rel.ID1])\n",
        "    | hl.is_defined(ancestry_to_remove[rel.ID2])\n",
        "    | hl.is_defined(withdrawn_to_remove[rel.ID1])\n",
        "    | hl.is_defined(withdrawn_to_remove[rel.ID2]),\n",
        "    keep=False,\n",
        ")\n",
        "\n",
        "related_samples_to_remove = rel.filter(rel.Kinship > MAX_KINSHIP, keep=True)\n",
        "\n",
        "print(\n",
        "    f\"Related samples not already in filter and high kinship coefficient: {related_samples_to_remove.count()}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ae865841-797d-4be0-a376-a4dc2d7c1b6b",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-13 08:55:41.864 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:55:48.920 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:55:49.544 Hail: INFO: Coerced sorted dataset\n",
            "2023-09-13 08:55:50.925 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 08:55:51.504 Hail: INFO: Coerced sorted dataset\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final number of samples to remove: 59423\n"
          ]
        }
      ],
      "source": [
        "final_to_remove = (\n",
        "    filtered_samples_to_remove.join(related_samples_to_remove, how=\"outer\")\n",
        "    .join(ancestry_to_remove, how=\"outer\")\n",
        "    .join(withdrawn_to_remove, how=\"outer\")\n",
        ").distinct()\n",
        "\n",
        "print(f\"Final number of samples to remove: {final_to_remove.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "231d5a18-dbaa-480d-9add-dedef378842b",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-13 09:22:30.867 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 09:22:37.555 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 09:22:38.145 Hail: INFO: Coerced sorted dataset\n",
            "2023-09-13 09:22:39.246 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2023-09-13 09:22:39.824 Hail: INFO: Coerced sorted dataset\n",
            "2023-09-13 09:22:44.635 Hail: INFO: merging 13 files totalling 464.2K...        \n",
            "2023-09-13 09:22:44.699 Hail: INFO: while writing:\n",
            "    /tmp/samples_to_remove.tsv\n",
            "  merge time: 63.099ms\n"
          ]
        }
      ],
      "source": [
        "SAMPLES_TO_REMOVE_FILE = \"/tmp/samples_to_remove.tsv\"\n",
        "\n",
        "final_to_remove.eid.export(SAMPLES_TO_REMOVE_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36529fbb-579e-4ab6-b799-a30a48a6f6fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export \n",
        "!hadoop fs -getmerge /tmp/samples_to_remove.tsv ../tmp/samples_to_remove.tsv\n",
        "!dx upload ../tmp/samples_to_remove.tsv --path /Data/Regenie/Input_regenie/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
