{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "18109b70",
      "metadata": {},
      "source": [
        "# SCRIPT TO PERFORM QUALITY CONTROL ON UK BIOBANK SAMPLES\n",
        "\n",
        "## This script should only be run once\n",
        "\n",
        "#### Initialization\n",
        "##### Load packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4366a523-07d4-487e-9595-a8949aaf127d",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import dxdata\n",
        "import dxpy\n",
        "import networkx as nx\n",
        "import hail as hl\n",
        "import pyspark\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "from fields import fields_for_id\n",
        "\n",
        "from packaging import version"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d5fa683",
      "metadata": {},
      "source": [
        "##### Spark, Hail and dataset configuration "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e49bc20a",
      "metadata": {},
      "outputs": [],
      "source": [
        "sc = pyspark.SparkContext()\n",
        "spark = SparkSession(sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "28b5e832-faba-408f-924c-a4b5d46e56f4",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "DATABASE = \"matrix_tables\"\n",
        "REFERENCE_GENOME = \"GRCh38\"\n",
        "\n",
        "LOG_FILE = (\n",
        "    Path(\"../hail_logs\", f\"hail_{datetime.now().strftime('%H%M')}.log\")\n",
        "    .resolve()\n",
        "    .__str__()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cf7eb9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "hl.init(sc=sc, default_reference=REFERENCE_GENOME, log=LOG_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "487d857f-9c15-41fe-b799-00b418258f60",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "dispensed_dataset_id = dxpy.find_one_data_object(\n",
        "    typename=\"Dataset\", name=\"app*.dataset\", folder=\"/\", name_mode=\"glob\"\n",
        ")[\"id\"]\n",
        "\n",
        "dataset = dxdata.load_dataset(id=dispensed_dataset_id)  # type: ignore\n",
        "participant = dataset[\"participant\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a09f964a",
      "metadata": {},
      "source": [
        "### Filtering\n",
        "#### Hard filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b33473f8-5a72-4edf-be1c-c6d0ab8b073b",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "fields = [\"22027\", # Outliers for heterozygosity or missing rate\n",
        "          \"22019\", # Sex chromosome aneuploidy\n",
        "          \"22021\", # Genetic kinship to other participants\n",
        "          \"21000\"] # Ethnic background\n",
        "\n",
        "field_names = [fields_for_id(i, participant) for i in fields]\n",
        "field_names = [\"eid\"] + [field.name for fields in field_names for field in fields]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37520b58",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = participant.retrieve_fields(\n",
        "    names=field_names,\n",
        "    engine=dxdata.connect(),\n",
        "    coding_values=\"replace\",  # type: ignore\n",
        ")\n",
        "\n",
        "df_filtered = df.filter(\n",
        "    (~df.p22027.isNull())\n",
        "    | (~df.p22019.isNull())\n",
        "    | (df.p22021 == \"Participant excluded from kinship inference process\")\n",
        "    | (df.p22021 == \"Ten or more third-degree relatives identified\")\n",
        "    | (df.p21000_i0 == \"White and Black Caribbean\")\n",
        "    | (df.p21000_i0 == \"White and Black African\")\n",
        "    | (df.p21000_i0 == \"White and Asian\")\n",
        "    | (df.p21000_i0 == \"Any other mixed background\")\n",
        ")\n",
        "filtered_samples_to_remove = hl.Table.from_spark(df_filtered.select(\"eid\")).key_by(\"eid\")\n",
        "\n",
        "print(f\"Samples to be filtered: {filtered_samples_to_remove.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0f89c54",
      "metadata": {},
      "source": [
        "#### Ancestry filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6186e22",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use ancestry filter from Privet et al.\n",
        "# In order to obtain this file, run files 01_QC_samples_pcs.ipynb and 01_QC_samples_ancestry.ipynb\n",
        "ANCESTRY_FILE = \"file:///opt/notebooks/ancestry.csv\"\n",
        "\n",
        "anc = hl.import_table(ANCESTRY_FILE, delimiter=\",\", quote='\"')\n",
        "anc = anc.key_by(eid=anc[\"PC_UKBB.eid\"])\n",
        "ancestry_to_remove = anc.filter(anc.group != \"United Kingdom\")\n",
        "\n",
        "print(f\"Ancestry to remove: {ancestry_to_remove.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fe634de",
      "metadata": {},
      "source": [
        "#### Withdrawn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa33964f",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_withdrawn = df.filter(df.eid.startswith(\"w\"))\n",
        "\n",
        "withdrawn_to_remove = hl.Table.from_spark(df_withdrawn.select(\"eid\")).key_by(\"eid\")\n",
        "print(f\"Withdrawn samples to remove: {withdrawn_to_remove.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f79fed29",
      "metadata": {},
      "source": [
        "#### Related individuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5d28ad9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove related individuals\n",
        "RAW_REL_FILE = Path(\"/mnt/project/Bulk/Genotype Results/Genotype calls/ukb_rel.dat\")\n",
        "MAX_KINSHIP = 0.125  # 2nd degree relatives \n",
        "\n",
        "rel = hl.import_table(\n",
        "    f\"file://{RAW_REL_FILE}\",\n",
        "    delimiter=\" \",\n",
        "    impute=True,\n",
        "    types={\"ID1\": \"str\", \"ID2\": \"str\"},\n",
        ")\n",
        "\n",
        "rel = rel.filter(\n",
        "    hl.is_defined(filtered_samples_to_remove[rel.ID1])\n",
        "    | hl.is_defined(filtered_samples_to_remove[rel.ID2])\n",
        "    | hl.is_defined(ancestry_to_remove[rel.ID1])\n",
        "    | hl.is_defined(ancestry_to_remove[rel.ID2])\n",
        "    | hl.is_defined(withdrawn_to_remove[rel.ID1])\n",
        "    | hl.is_defined(withdrawn_to_remove[rel.ID2]),\n",
        "    keep=False,\n",
        ")\n",
        "\n",
        "rel = rel.filter(rel.Kinship > MAX_KINSHIP, keep=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fd79a4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hail maximal independent set is not working so we use networkx \n",
        "\n",
        "# Collect the ID pairs into a list for processing\n",
        "rel_data = rel.select('ID1', 'ID2').collect()\n",
        "\n",
        "# Create a graph using networkx from the relationships\n",
        "G = nx.Graph()\n",
        "for row in rel_data:\n",
        "    G.add_edge(row.ID1, row.ID2)\n",
        "\n",
        "# Compute the maximal independent set using networkx\n",
        "independent_set = set(nx.maximal_independent_set(G))\n",
        "\n",
        "# Extract all unique IDs from the original relationship data\n",
        "all_ids = set([row.ID1 for row in rel_data] + [row.ID2 for row in rel_data])\n",
        "\n",
        "# Calculate the related samples to remove (those not in the independent set)\n",
        "related_samples_to_remove_ids = all_ids - independent_set\n",
        "\n",
        "# Convert the related samples to remove into a Hail Table and key the table by \"eid\"\n",
        "related_samples_to_remove = hl.Table.parallelize(\n",
        "    [hl.struct(eid=sample) for sample in related_samples_to_remove_ids]\n",
        ").key_by('eid')\n",
        "\n",
        "print(f\"Related samples not already in filter and high kinship coefficient: {related_samples_to_remove.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e7bfeca",
      "metadata": {},
      "source": [
        "#### Combine all samples to remove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa596461",
      "metadata": {},
      "outputs": [],
      "source": [
        "final_to_remove = (\n",
        "    filtered_samples_to_remove.join(ancestry_to_remove, how=\"outer\")\n",
        "    .join(withdrawn_to_remove, how=\"outer\")\n",
        "    .join(related_samples_to_remove, how=\"outer\")\n",
        ").distinct()\n",
        "\n",
        "print(f\"Final number of samples to remove: {final_to_remove.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f0dc020",
      "metadata": {},
      "source": [
        "### Save and export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3832fd3c",
      "metadata": {},
      "outputs": [],
      "source": [
        "SAMPLES_TO_REMOVE_FILE = \"/tmp/samples_to_remove.tsv\"\n",
        "\n",
        "final_to_remove.eid.export(SAMPLES_TO_REMOVE_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dadf2951",
      "metadata": {},
      "outputs": [],
      "source": [
        "!hadoop fs -getmerge /tmp/samples_to_remove.tsv ../tmp/samples_to_remove.tsv\n",
        "!dx upload ../tmp/samples_to_remove.tsv --path WGS_Javier/Data/Input_regenie/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
