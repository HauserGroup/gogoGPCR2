{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "18109b70",
      "metadata": {},
      "source": [
        "# SCRIPT TO PERFORM QUALITY CONTROL ON UK BIOBANK SAMPLES\n",
        "Ancestry and kinship and such\n",
        "Could use an upgrade\n",
        "\n",
        "## This script should only be run once\n",
        "\n",
        "#### Initialization\n",
        "##### Load packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4366a523-07d4-487e-9595-a8949aaf127d",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>\n",
              "        .bk-notebook-logo {\n",
              "            display: block;\n",
              "            width: 20px;\n",
              "            height: 20px;\n",
              "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
              "        }\n",
              "    </style>\n",
              "    <div>\n",
              "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
              "        <span id=\"p1001\">Loading BokehJS ...</span>\n",
              "    </div>\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n    // Clean up Bokeh references\n    if (id != null && id in Bokeh.index) {\n      Bokeh.index[id].model.document.clear();\n      delete Bokeh.index[id];\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim();\n            if (id in Bokeh.index) {\n              Bokeh.index[id].model.document.clear();\n              delete Bokeh.index[id];\n            }\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"p1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.0.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.0.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.0.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.0.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.0.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"p1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
            "application/vnd.bokehjs_load.v0+json": ""
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "import dxdata  # type: ignore\n",
        "import dxpy\n",
        "import networkx as nx\n",
        "import hail as hl\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "# --- HELPER FUNCTIONS ---\n",
        "\n",
        "\n",
        "def fields_for_id(field_id, participant):\n",
        "    \"\"\"Retrieve and sort fields, robustly handling _i and _a indices.\"\"\"\n",
        "    fid_str = str(field_id)\n",
        "    fields = participant.find_fields(name_regex=rf\"^p{fid_str}(_i\\d+)?(_a\\d+)?$\")\n",
        "\n",
        "    def get_sort_key(field):\n",
        "        i_match = re.search(r\"_i(\\d+)\", field.name)\n",
        "        a_match = re.search(r\"_a(\\d+)\", field.name)\n",
        "        return (\n",
        "            int(i_match.group(1)) if i_match else 0,\n",
        "            int(a_match.group(1)) if a_match else 0,\n",
        "        )\n",
        "\n",
        "    return sorted(fields, key=get_sort_key)\n",
        "\n",
        "\n",
        "def get_primary_column(df, field_id):\n",
        "    \"\"\"Finds the Instance 0 column (pXXXX_i0) for a field.\"\"\"\n",
        "    candidates = [c for c in df.columns if c.startswith(f\"p{field_id}\")]\n",
        "    # Sort by length and name to prioritize 'p54' or 'p54_i0' over 'p54_i1'\n",
        "    candidates.sort(key=lambda x: (len(x), x))\n",
        "    if not candidates:\n",
        "        raise ValueError(f\"Field {field_id} not found in dataframe\")\n",
        "    return candidates[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d5fa683",
      "metadata": {},
      "source": [
        "##### Spark, Hail and dataset configuration "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "28b5e832-faba-408f-924c-a4b5d46e56f4",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.12/site-packages/hail/context.py:352: UserWarning:\n",
            "\n",
            "Using hl.init with a default_reference argument is deprecated. To set a default reference genome after initializing hail, call `hl.default_reference` with an argument to set the default reference genome.\n",
            "\n",
            "pip-installed Hail requires additional configuration options in Spark referring\n",
            "  to the path to the Hail Python module directory HAIL_DIR,\n",
            "  e.g. /path/to/python/site-packages/hail:\n",
            "    spark.jars=HAIL_DIR/backend/hail-all-spark.jar\n",
            "    spark.driver.extraClassPath=HAIL_DIR/backend/hail-all-spark.jar\n",
            "    spark.executor.extraClassPath=./hail-all-spark.jarRunning on Apache Spark version 3.5.2\n",
            "SparkUI available at http://ip-10-60-122-101.eu-west-2.compute.internal:8081\n",
            "Welcome to\n",
            "     __  __     <>__\n",
            "    / /_/ /__  __/ /\n",
            "   / __  / _ `/ / /\n",
            "  /_/ /_/\\_,_/_/_/   version 0.2.132-678e1f52b999\n",
            "LOGGING: writing to /opt/hail_logs/hail_1543.log\n",
            "2025-12-19 15:45:49.257 Hail: INFO: Reading table to impute column types\n",
            "2025-12-19 15:45:50.827 Hail: INFO: Finished type imputation\n",
            "  Loading field 'ID1' as type str (user-supplied type)\n",
            "  Loading field 'ID2' as type str (user-supplied type)\n",
            "  Loading field 'HetHet' as type float64 (imputed)\n",
            "  Loading field 'IBS0' as type float64 (imputed)\n",
            "  Loading field 'Kinship' as type float64 (imputed)\n",
            "2025-12-19 15:47:03.118 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2025-12-19 15:47:03.685 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2025-12-19 15:47:17.544 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2025-12-19 15:47:17.960 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2025-12-19 15:47:21.237 Hail: INFO: merging 82 files totalling 380.4K...\n",
            "2025-12-19 15:47:21.413 Hail: INFO: while writing:\n",
            "    /tmp/samples_to_remove.tsv\n",
            "  merge time: 174.972ms\n",
            "2025-12-19 15:47:36.298 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2025-12-19 15:47:36.623 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2025-12-19 15:47:38.980 Hail: INFO: merging 82 files totalling 380.4K...\n",
            "2025-12-19 15:47:39.096 Hail: INFO: while writing:\n",
            "    /tmp/samples_to_remove.tsv\n",
            "  merge time: 115.807ms\n",
            "2025-12-19 15:52:21.863 Hail: INFO: Reading table to impute column types\n",
            "2025-12-19 15:52:22.660 Hail: INFO: Finished type imputation\n",
            "  Loading field 'ID1' as type str (user-supplied type)\n",
            "  Loading field 'ID2' as type str (user-supplied type)\n",
            "  Loading field 'HetHet' as type float64 (imputed)\n",
            "  Loading field 'IBS0' as type float64 (imputed)\n",
            "  Loading field 'Kinship' as type float64 (imputed)\n",
            "2025-12-19 15:52:33.097 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2025-12-19 15:52:34.378 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2025-12-19 15:52:36.355 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2025-12-19 15:52:37.583 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2025-12-19 15:53:22.735 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2025-12-19 15:53:23.084 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2025-12-19 15:53:57.854 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2025-12-19 15:53:58.138 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
            "2025-12-19 15:54:00.284 Hail: INFO: merging 82 files totalling 380.0K...\n",
            "2025-12-19 15:54:00.391 Hail: INFO: while writing:\n",
            "    /tmp/samples_to_remove.tsv\n",
            "  merge time: 106.511ms\n"
          ]
        }
      ],
      "source": [
        "# Constants\n",
        "# Run once only\n",
        "DATABASE = \"matrix_tables\"\n",
        "REFERENCE_GENOME = \"GRCh38\"\n",
        "\n",
        "LOG_FILE = (\n",
        "    Path(\"../hail_logs\", f\"hail_{datetime.now().strftime('%H%M')}.log\")\n",
        "    .resolve()\n",
        "    .__str__()\n",
        ")\n",
        "\n",
        "sc = pyspark.SparkContext()\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "hl.init(sc=sc, default_reference=REFERENCE_GENOME, log=LOG_FILE)\n",
        "\n",
        "dispensed_dataset_id = dxpy.find_one_data_object(\n",
        "    typename=\"Dataset\", name=\"app*.dataset\", folder=\"/\", name_mode=\"glob\"\n",
        ")[\"id\"]\n",
        "\n",
        "dataset = dxdata.load_dataset(id=dispensed_dataset_id)  # type: ignore\n",
        "participant = dataset[\"participant\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a09f964a",
      "metadata": {},
      "source": [
        "### Filtering\n",
        "#### Hard filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b33473f8-5a72-4edf-be1c-c6d0ab8b073b",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "use_ancestry = True  # You should argue this\n",
        "\n",
        "fields = [\n",
        "    \"22027\",  # Outliers for heterozygosity or missing rate\n",
        "    \"22019\",  # Sex chromosome aneuploidy\n",
        "    \"22021\",  # Genetic kinship to other participants\n",
        "]\n",
        "\n",
        "if use_ancestry:\n",
        "    fields = fields + [\"30079\"]  #  Genomic ancestry (Pan-UKB)\n",
        "\n",
        "field_names = [fields_for_id(i, participant) for i in fields]\n",
        "field_names = [\"eid\"] + [field.name for fields in field_names for field in fields]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "37520b58",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples to be filtered: 22896\n"
          ]
        }
      ],
      "source": [
        "df = participant.retrieve_fields(\n",
        "    names=field_names,\n",
        "    engine=dxdata.connect(),\n",
        "    coding_values=\"replace\",  # type: ignore\n",
        ")\n",
        "\n",
        "df_filtered = df.filter(\n",
        "    (~df.p22027.isNull())\n",
        "    | (~df.p22019.isNull())\n",
        "    | (df.p22021 == \"Participant excluded from kinship inference process\")\n",
        "    | (df.p22021 == \"Ten or more third-degree relatives identified\")\n",
        "    | (\n",
        "        df.p30079 != \"European ancestry (EUR)\"  # proooblematic\n",
        "    )\n",
        ")\n",
        "\n",
        "filtered_samples_to_remove = hl.Table.from_spark(df_filtered.select(\"eid\")).key_by(\n",
        "    \"eid\"\n",
        ")\n",
        "\n",
        "print(f\"Samples to be filtered: {filtered_samples_to_remove.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0f89c54",
      "metadata": {},
      "source": [
        "#### Ancestry filtering\n",
        "Now as field 30079 from Pan-UKB"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fe634de",
      "metadata": {},
      "source": [
        "#### Withdrawn\n",
        "Now handled automatically by UKB"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f79fed29",
      "metadata": {},
      "source": [
        "#### Related individuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "d5d28ad9",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Remove related individuals\n",
        "RAW_REL_FILE = Path(\"/mnt/project/Bulk/Genotype Results/Genotype calls/ukb_rel.dat\")\n",
        "MAX_KINSHIP = 0.176  # 2nd degree relatives, based on: https://kenhanscombe.github.io/ukbkings/reference/bio_gen_related_remove.html\n",
        "\n",
        "rel = hl.import_table(\n",
        "    f\"file://{RAW_REL_FILE}\",\n",
        "    delimiter=\" \",\n",
        "    impute=True,\n",
        "    types={\"ID1\": \"str\", \"ID2\": \"str\"},\n",
        ")\n",
        "\n",
        "rel = rel.filter(\n",
        "    hl.is_defined(filtered_samples_to_remove[rel.ID1])\n",
        "    | hl.is_defined(filtered_samples_to_remove[rel.ID2]),\n",
        "    keep=False,\n",
        ")\n",
        "\n",
        "rel = rel.filter(rel.Kinship > MAX_KINSHIP, keep=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "3fd79a4f",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Related samples not already in filter and high kinship coefficient: 25746\n"
          ]
        }
      ],
      "source": [
        "# Hail maximal independent set is not working so we use networkx\n",
        "# related_samples_to_remove = (hl.maximal_independent_set(i=rel.ID1, j=rel.ID2,keep=False).rename({\"node\": \"eid\"}).key_by(\"eid\"))\n",
        "\n",
        "# Collect the ID pairs into a list for processing\n",
        "rel_data = rel.select(\"ID1\", \"ID2\").collect()\n",
        "\n",
        "# Create a graph using networkx from the relationships\n",
        "G = nx.Graph()\n",
        "for row in rel_data:\n",
        "    G.add_edge(row.ID1, row.ID2)\n",
        "\n",
        "# Compute the maximal independent set using networkx\n",
        "independent_set = set(nx.maximal_independent_set(G))\n",
        "\n",
        "# Extract all unique IDs from the original relationship data\n",
        "all_ids = set([row.ID1 for row in rel_data] + [row.ID2 for row in rel_data])\n",
        "\n",
        "# Calculate the related samples to remove (those not in the independent set)\n",
        "related_samples_to_remove_ids = all_ids - independent_set\n",
        "\n",
        "# Convert the related samples to remove into a Hail Table and key the table by \"eid\"\n",
        "related_samples_to_remove = hl.Table.parallelize(\n",
        "    [hl.struct(eid=sample) for sample in related_samples_to_remove_ids]\n",
        ").key_by(\"eid\")\n",
        "\n",
        "print(\n",
        "    f\"Related samples not already in filter and high kinship coefficient: {related_samples_to_remove.count()}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e7bfeca",
      "metadata": {},
      "source": [
        "#### Combine all samples to remove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "fa596461",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final number of samples to remove: 48642\n"
          ]
        }
      ],
      "source": [
        "final_to_remove = (\n",
        "    filtered_samples_to_remove.join(related_samples_to_remove, how=\"outer\")\n",
        "    # .join(withdrawn_to_remove, how=\"outer\")\n",
        ").distinct()\n",
        "\n",
        "print(f\"Final number of samples to remove: {final_to_remove.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f0dc020",
      "metadata": {},
      "source": [
        "### Save and export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "3832fd3c",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "SAMPLES_TO_REMOVE_FILE = \"/tmp/samples_to_remove.tsv\"\n",
        "\n",
        "final_to_remove.eid.export(SAMPLES_TO_REMOVE_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "dadf2951",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-19 15:54:06,884 WARN metrics.MetricsReporter: Unable to initialize metrics scraping configurations from hive-site.xml. Message:InputStream cannot be null\n",
            "2025-12-19 15:54:06,986 WARN service.DNAxApiSvc: Using default configurations. Unable to find dnanexus.conf.location=null\n",
            "2025-12-19 15:54:06,986 INFO service.DNAxApiSvc: apiserver connection-pool config. MaxPoolSize=10, MaxPoolPerRoute=10,MaxWaitTimeout=60000\n",
            "2025-12-19 15:54:06,986 INFO service.DNAxApiSvc: initializing http connection manager pools\n",
            "2025-12-19 15:54:07,534 INFO service.DNAxApiSvc: Worker process - IdleConnectionMonitorThread disabled\n",
            "2025-12-19 15:54:07,534 INFO service.DNAxApiSvc: Worker process - IdleConnectionMonitorThread disabled\n",
            "2025-12-19 15:54:07,534 INFO service.DNAxApiSvc: initializing DNAxApiSvc\n",
            "2025-12-19 15:54:08,077 WARN service.DNAxApiSvc: Shutting down Runtime service for Connection Pools\n",
            "2025-12-19 15:54:08,078 INFO service.DNAxApiSvc: shutting down httpClientConnManager\n",
            "2025-12-19 15:54:08,079 INFO service.DNAxApiSvc: shutting down httpsClientConnManager\n",
            "[===========================================================>] Uploaded 389,140 of 389,140 bytes (100%) ../tmp/samples_to_remove.tsv\n",
            "ID                                file-J52gB88Jb4JBJX320KYJy77G\n",
            "Class                             file\n",
            "Project                           project-GfVK998Jb4JJgVBjKXPyxJ9q\n",
            "Folder                            /TASR/Phenotypes/QC\n",
            "Name                              samples_to_remove.tsv\n",
            "State                             \u001b[33mclosing\u001b[0m\n",
            "Visibility                        visible\n",
            "Types                             -\n",
            "Properties                        -\n",
            "Tags                              -\n",
            "Outgoing links                    -\n",
            "Created                           Fri Dec 19 15:54:09 2025\n",
            "Created by                        jsmadsen\n",
            " via the job                      job-J52b7QQJb4J5kKGx6y7X830K\n",
            "Last modified                     Fri Dec 19 15:54:11 2025\n",
            "Media type                        \n",
            "archivalState                     \"live\"\n",
            "cloudAccount                      \"cloudaccount-dnanexus\"\n"
          ]
        }
      ],
      "source": [
        "!hadoop fs -getmerge /tmp/samples_to_remove.tsv ../tmp/samples_to_remove.tsv\n",
        "!dx upload ../tmp/samples_to_remove.tsv --path TASR/Phenotypes/QC/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "484a52e6-a3f6-4884-b5ac-9fd3be2b5344",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
