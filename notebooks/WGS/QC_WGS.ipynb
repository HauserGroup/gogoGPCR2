{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRIPT TO GENERATE COVARIATES\n",
    "\n",
    "## This script should be only run once\n",
    "\n",
    "In order to run, there has to be several files in the project folder:\n",
    "- GENCODE GTF: Obtain from: https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_46/gencode.v46.annotation.gtf.gz (Check for newer versions)\n",
    "\n",
    "In order to obtain this file, run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "GENCODE_GTF = \"https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_46/gencode.v46.annotation.gtf.gz\"\n",
    "\n",
    "if not Path(\"/mnt/project/WGS_Javier/WGS_QC/gencode.v46.annotation.gtf.gz\").exists():\n",
    "    response = requests.get(GENCODE_GTF)\n",
    "    if response.status_code == 200:\n",
    "        with open(Path(\"/tmp/gencode.v46.annotation.gtf.gz\"), \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "!dx upload /tmp/gencode.v46.annotation.gtf.gz --path /WGS_Javier/WGS_QC/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file should be unziped\n",
    "In order to do so, use Swiss-army-knife with the following command: unzip -d gencode.v46.annotation.gtf.gz \n",
    "\n",
    "Once completed, a new Jupyter Notebook should be initialized so we can access this file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- PVCF BLOCKS: https://biobank.ndph.ox.ac.uk/ukb/ukb/auxdata/dragen_pvcf_coordinates.zip Obtain from:\n",
    "It needs parsing, but in data/misc it is already parsed\n",
    "\n",
    "\n",
    "#### Initialization \n",
    "##### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pyspark\n",
    "import dxpy\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from src.matrixtables import import_mt, smart_split_multi_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATABASE = \"matrix_tables\"\n",
    "REFERENCE_GENOME = \"GRCh38\"\n",
    "PROJ_NAME = \"GIPR_test\"\n",
    "\n",
    "# RAP\n",
    "VCF_VERSION = \"v1\"\n",
    "FIELD_ID = 24310\n",
    "\n",
    "# Paths\n",
    "BULK_DIR = Path(\"/mnt/project/Bulk\")\n",
    "VCF_DIR = Path(\"DRAGEN WGS/DRAGEN population level WGS variants, pVCF format 500k release\")\n",
    "INTERVAL_FILE = Path(\"Exome sequences/Exome OQFE CRAM files/helper_files/xgen_plus_spikein.GRCh38.bed\")\n",
    "MISC_DIR = Path(\"/mnt/project/WGS_QC/\")\n",
    "\n",
    "# Genes\n",
    "GENES = [\"GIPR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"/tmp\").resolve().mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOG_FILE = (\n",
    "    Path(\"../hail_logs\", f\"{PROJ_NAME}_{datetime.now().strftime('%H%M')}.log\")\n",
    "    .resolve()\n",
    "    .__str__()\n",
    ")\n",
    "\n",
    "# Spark init\n",
    "sc = pyspark.SparkContext()\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "\n",
    "# Create database in DNAX\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE} LOCATION 'dnax://'\")\n",
    "mt_database = dxpy.find_one_data_object(name=DATABASE)[\"id\"]\n",
    "\n",
    "# Hail init\n",
    "hl.init(sc=sc, default_reference=REFERENCE_GENOME, log=LOG_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gene intervals\n",
    "gene_interval = hl.experimental.get_gene_intervals(\n",
    "    gene_symbols=GENES,\n",
    "    reference_genome=\"GRCh38\",\n",
    "    gtf_file=\"file:///mnt/project/WGS_Javier/WGS_QC/gencode.v46.annotation.gtf\",\n",
    ")\n",
    "gene_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DRAGEN pVCF blocks\n",
    "blocks = hl.import_table(\"file:///mnt/project/WGS_Javier/WGS_QC/dragen_pvcf_blocks.tsv\", no_header=False)\n",
    "blocks = blocks.annotate(Chromosome=blocks.Chromosome.replace(\"23\", \"X\").replace(\"24\", \"Y\"))\n",
    "blocks = blocks.annotate(region=hl.str(\"\").join([hl.str(\"chr\"), blocks.Chromosome]))\n",
    "blocks = blocks.annotate(\n",
    "    interval=hl.locus_interval(\n",
    "        blocks.region,\n",
    "        hl.int32(blocks.Starting_Position),\n",
    "        hl.int32(blocks.Ending_Position),\n",
    "        reference_genome=\"GRCh38\",\n",
    "    )\n",
    ").key_by(\"interval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for the given genes\n",
    "gb = blocks.filter(hl.any(lambda inter: blocks.interval.overlaps(inter), gene_interval))\n",
    "\n",
    "gb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET VCF FILES\n",
    "vcf_files = [\n",
    "    f\"file://{BULK_DIR / VCF_DIR}/{chromosome}/ukb{FIELD_ID}_c{chromosome.replace('chr', '')}_b{block}_{VCF_VERSION}.vcf.gz\"\n",
    "    for block, chromosome in zip(gb.f2.collect(), gb.region.collect())\n",
    "]\n",
    "\n",
    "mt = hl.import_vcf(\n",
    "    vcf_files,\n",
    "    drop_samples=False,\n",
    "    reference_genome=\"GRCh38\",\n",
    "    array_elements_required=False,\n",
    "    force_bgz=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only genes of interest\n",
    "mt = hl.filter_intervals(mt, gene_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only exome capture region\n",
    "interval_table = hl.import_bed(\n",
    "    f\"file://{BULK_DIR / INTERVAL_FILE}\",\n",
    "    reference_genome=\"GRCh38\",\n",
    ")\n",
    "\n",
    "mt = mt.filter_rows(hl.is_defined(interval_table[mt.locus]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{mt.count_rows()} variants after interval filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First checkpoint\n",
    "stage = \"FIRST\"\n",
    "checkpoint_file = f\"/tmp/{PROJ_NAME}.{stage}.cp.mt\"\n",
    "\n",
    "mt = mt.checkpoint(checkpoint_file, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi allele filtering\n",
    "mt = mt.filter_rows(mt.alleles.length() <= 6)\n",
    "mt = smart_split_multi_mt(mt)\n",
    "\n",
    "print(f\"{mt.count_rows()} variants with not more than 6 alleles after splitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant effect predictor \n",
    "VEP_JSON = Path(\"GRCh38_VEP.json\").resolve()\n",
    "\n",
    "mt = hl.vep(mt, f\"file:{VEP_JSON}\")\n",
    "\n",
    "mt = mt.annotate_rows(**mt.vep)\n",
    "mt = mt.annotate_rows(**mt.transcript_consequences[0])\n",
    "mt = mt.annotate_rows(\n",
    "    protCons=mt.amino_acids.split(\"/\")[0]\n",
    "    + hl.str(mt.protein_end)\n",
    "    + mt.amino_acids.split(\"/\")[-1],\n",
    "    varid=hl.variant_str(mt.locus, mt.alleles),\n",
    ")\n",
    "\n",
    "mt = mt.drop(\"vep\", \"transcript_consequences\", \"vep_proc_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second checkpoint\n",
    "stage = \"SECOND\"\n",
    "checkpoint_file = f\"/tmp/{PROJ_NAME}.{stage}.cp.mt\"\n",
    "\n",
    "mt = mt.checkpoint(checkpoint_file, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove samples\n",
    "SAMPLES_TO_REMOVE_FILE = Path('samples_to_remove.tsv').resolve()\n",
    "!hadoop fs -put {SAMPLES_TO_REMOVE_FILE} /tmp\n",
    "\n",
    "samples_to_remove = hl.import_table(f\"/tmp/{SAMPLES_TO_REMOVE_FILE.name}\", key=\"eid\")\n",
    "samples_to_remove.count()\n",
    "\n",
    "mt = mt.anti_join_cols(samples_to_remove)\n",
    "\n",
    "print(f\"Samples remaining after hard filtering samples: {mt.count_cols()} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not working\n",
    "\n",
    "#mt = mt.annotate_entries(AB=(mt.LAD[1] / hl.sum(mt.LAD)))\n",
    "\n",
    "#filter_condition_ab = (\n",
    "#    (mt.GT.is_hom_ref() & (mt.AB <= 0.1))\n",
    "#    | (mt.GT.is_het() & (mt.AB >= 0.25) & (mt.AB <= 0.75))\n",
    "#    | (mt.GT.is_hom_var() & (mt.AB >= 0.9))\n",
    "#)\n",
    "\n",
    "#mt = mt.filter_entries(filter_condition_ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = hl.variant_qc(mt)\n",
    "mt = mt.filter_rows(mt.variant_qc.gq_stats.mean >= 20)\n",
    "#mt = mt.filter_rows(mt.variant_qc.dp_stats.mean >= 12)\n",
    "mt = mt.filter_rows(mt.variant_qc.call_rate >= 0.95)\n",
    "mt = mt.filter_rows(mt.variant_qc.n_non_ref > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third checkpoint\n",
    "stage = \"THIRD\"\n",
    "checkpoint_file = f\"/tmp/{PROJ_NAME}.{stage}.cp.mt\"\n",
    "\n",
    "mt = mt.checkpoint(checkpoint_file, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt = mt.rows()\n",
    "\n",
    "qt = qt.select(\n",
    "    qt.varid,\n",
    "    qt.protCons,\n",
    "    qt.most_severe_consequence,\n",
    "    qt.protein_end,\n",
    "    qt.protein_start,\n",
    "    qt.amino_acids,\n",
    "    qt.gene_id,\n",
    "    qt.transcript_id,\n",
    "    **qt.variant_qc.flatten(),\n",
    ")\n",
    "qt = qt.annotate(AC=qt.AC[1], AF=qt.AF[1], homozygote_count=qt.homozygote_count[1])\n",
    "qt = qt.key_by().drop(\"locus\", \"alleles\")\n",
    "\n",
    "qt.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt.export(\"/tmp/variant_qc.tsv\")\n",
    "!hadoop fs -getmerge /tmp/variant_qc.tsv ../variant_qc.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BGEN file\n",
    "BGEN_FILE = \"/tmp/GIPR_test\"\n",
    "GPs = hl.literal([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]])\n",
    "\n",
    "mt = mt.annotate_entries(GP=GPs[mt.GT.n_alt_alleles()])\n",
    "\n",
    "hl.export_bgen(\n",
    "    mt=mt, varid=mt.varid, rsid=mt.varid, gp=mt.GP, output=\"file:\" + BGEN_FILE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANNOTATIONS file\n",
    "ANNOTATIONS_FILE = \"/tmp/GIPR_test.annotations\"\n",
    "\n",
    "annotations = (\n",
    "    mt.select_rows(\n",
    "        varid=mt.varid,\n",
    "        gene=mt.gene_id,\n",
    "        annotation=mt.protCons,\n",
    "    )\n",
    "    .rows()\n",
    "    .key_by(\"varid\")\n",
    "    .drop(\"locus\")\n",
    "    .drop(\"alleles\")\n",
    ")\n",
    "\n",
    "annotations.export(\"file:\" + ANNOTATIONS_FILE, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETLIST file\n",
    "SETLIST_FILE = \"/tmp/GIPR_test.setlist\"\n",
    "position = mt.aggregate_rows(hl.agg.min(mt.locus.position))\n",
    "names = mt.varid.collect()\n",
    "names_str = \",\".join(names)\n",
    "\n",
    "line = f\"{mt.gene_id.collect()[0]}\\t{mt.locus.contig.collect()[0]}\\t{position}\\t{names_str}\"\n",
    "\n",
    "with open(SETLIST_FILE, \"w\") as f:\n",
    "    f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgen_file = BGEN_FILE + \".bgen\"\n",
    "sample_file = BGEN_FILE + \".sample\"\n",
    "\n",
    "!dx upload bgen_file sample_file ANNOTATIONS_FILE SETLIST_FILE --path WGS_Javier/WGS_QC/Output/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
