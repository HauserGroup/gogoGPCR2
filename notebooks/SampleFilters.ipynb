{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4366a523-07d4-487e-9595-a8949aaf127d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "\n",
    "import dxdata\n",
    "import dxpy\n",
    "import hail as hl\n",
    "import pyspark\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "\n",
    "from packaging import version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b7c5057-4e18-43e3-b30f-40d1e8bbc23a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/cluster/dnax/jars/dnanexus-api-0.1.0-SNAPSHOT-jar-with-dependencies.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/cluster/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-13 08:36:00.385 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-09-13 08:36:01.462 WARN  Utils:69 - Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 43000. Attempting port 43001.\n",
      "2023-09-13 08:36:01.720 WARN  MetricsReporter:84 - No metrics configured for reporting\n",
      "2023-09-13 08:36:01.722 WARN  LineProtoUsageReporter:48 - Telegraf configurations: url [metrics.push.telegraf.hostport], user [metrics.push.telegraf.user] or password [metrics.push.telegraf.password] missing.\n",
      "2023-09-13 08:36:01.722 WARN  MetricsReporter:117 - metrics.scraping.httpserver.port\n"
     ]
    }
   ],
   "source": [
    "# Initialise Spark\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "spark = pyspark.sql.SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28b5e832-faba-408f-924c-a4b5d46e56f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATABASE = \"matrix_tables\"\n",
    "REFERENCE_GENOME = \"GRCh38\"\n",
    "PROJ_NAME = \"DRD2\"\n",
    "\n",
    "LOG_FILE = (\n",
    "    Path(\"../hail_logs\", f\"{PROJ_NAME}_{datetime.now().strftime('%H%M')}.log\")\n",
    "    .resolve()\n",
    "    .__str__()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1918ea4a-bad7-4468-9d4f-da96f3b6c723",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pip-installed Hail requires additional configuration options in Spark referring\n",
      "  to the path to the Hail Python module directory HAIL_DIR,\n",
      "  e.g. /path/to/python/site-packages/hail:\n",
      "    spark.jars=HAIL_DIR/backend/hail-all-spark.jar\n",
      "    spark.driver.extraClassPath=HAIL_DIR/backend/hail-all-spark.jar\n",
      "    spark.executor.extraClassPath=./hail-all-spark.jar"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log4j: Parsing for [root] with value=[INFO, logfile].\n",
      "log4j: Level token is [INFO].\n",
      "log4j: Category root set to INFO\n",
      "log4j: Parsing appender named \"logfile\".\n",
      "log4j: Parsing layout options for \"logfile\".\n",
      "log4j: Setting property [conversionPattern] to [%d{yyyy-MM-dd HH:mm:ss.SSS} %c{1}: %p: %m%n].\n",
      "log4j: End of parsing for \"logfile\".\n",
      "log4j: Setting property [append] to [false].\n",
      "log4j: Setting property [threshold] to [INFO].\n",
      "log4j: Setting property [file] to [/opt/notebooks/gogoGPCR2/hail_logs/DRD2_0836.log].\n",
      "log4j: setFile called: /opt/notebooks/gogoGPCR2/hail_logs/DRD2_0836.log, false\n",
      "log4j: setFile ended\n",
      "log4j: Parsed \"logfile\" options.\n",
      "log4j: Parsing for [Hail] with value=[INFO, HailSocketAppender].\n",
      "log4j: Level token is [INFO].\n",
      "log4j: Category Hail set to INFO\n",
      "log4j: Parsing appender named \"HailSocketAppender\".\n",
      "log4j: Parsed \"HailSocketAppender\" options.\n",
      "log4j: Handling log4j.additivity.Hail=[null]\n",
      "log4j: Finished configuring.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running on Apache Spark version 3.2.3\n",
      "SparkUI available at http://ip-10-60-22-64.eu-west-2.compute.internal:8081\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.116-cd64e0876c94\n",
      "LOGGING: writing to /opt/notebooks/gogoGPCR2/hail_logs/DRD2_0836.log\n"
     ]
    }
   ],
   "source": [
    "# Hail init\n",
    "hl.init(sc=sc, default_reference=REFERENCE_GENOME, log=LOG_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "487d857f-9c15-41fe-b799-00b418258f60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dispensed_database_name = dxpy.find_one_data_object(\n",
    "    classname=\"database\", name=\"app*\", folder=\"/\", name_mode=\"glob\", describe=True\n",
    ")[\"describe\"][\"name\"]\n",
    "dispensed_dataset_id = dxpy.find_one_data_object(\n",
    "    typename=\"Dataset\", name=\"app*.dataset\", folder=\"/\", name_mode=\"glob\"\n",
    ")[\"id\"]\n",
    "\n",
    "dataset = dxdata.load_dataset(id=dispensed_dataset_id)\n",
    "participant = dataset[\"participant\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b33473f8-5a72-4edf-be1c-c6d0ab8b073b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fields_for_id(field_id, participant):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    field_id : [type]\n",
    "        [description]\n",
    "    participant : [type]\n",
    "        [description]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    [type]\n",
    "        [description]\n",
    "    \"\"\"\n",
    "\n",
    "    field_id = str(field_id)\n",
    "    fields = participant.find_fields(\n",
    "        name_regex=r\"^p{}(_i\\d+)?(_a\\d+)?$\".format(field_id)\n",
    "    )\n",
    "    \n",
    "    return fields\n",
    "\n",
    "    return sorted(fields, key=lambda f: version.parse(f.name.replace(\"p\", \"\")))\n",
    "\n",
    "fields = [\"22027\", \"22019\", \"22021\", \"21000\"]\n",
    "\n",
    "field_names = [\n",
    "    fields_for_id(i, participant) for i in fields\n",
    "]\n",
    "\n",
    "field_names = [\"eid\"] + [field.name for fields in field_names for field in fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96cd863a-23c8-477f-956a-2c13a09f33ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=============================================>            (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples to be filtered: 4713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use hard filters\n",
    "\n",
    "df = participant.retrieve_fields(\n",
    "    names=field_names, engine=dxdata.connect(), coding_values=\"replace\"\n",
    ")\n",
    "\n",
    "df = df.filter(\n",
    "    (~df.p22027.isNull())\n",
    "    | (~df.p22019.isNull())\n",
    "    | (df.p22021 == \"Participant excluded from kinship inference process\")\n",
    "    | (df.p22021 == \"Ten or more third-degree relatives identified\")\n",
    "    | (df.p21000_i0 == \"White and Black Caribbean\")\n",
    "    | (df.p21000_i0 == \"White and Black African\")\n",
    "    | (df.p21000_i0 == \"White and Asian\")\n",
    "    | (df.p21000_i0 == \"Any other mixed background\")\n",
    ")\n",
    "filtered_samples_to_remove = hl.Table.from_spark(df.select(\"eid\")).key_by(\"eid\")\n",
    "\n",
    "print(f\"Samples to be filtered: {filtered_samples_to_remove.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "755362df-ad24-4161-86f6-081c15cdd655",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-13 08:36:39.239 Hail: INFO: Reading table without type imputation       \n",
      "  Loading field '' as type str (not specified)\n",
      "  Loading field 'PC_UKBB.eid' as type str (not specified)\n",
      "  Loading field 'group' as type str (not specified)\n",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ancestry to remove: 30171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use ancestry filter from Privet et al.\n",
    "ANCESTRY_FILE = 'file:///mnt/project/gogoGPCR2/ancestry.csv'\n",
    "\n",
    "anc = hl.import_table(ANCESTRY_FILE, delimiter = ',', quote = '\"')\n",
    "anc = anc.key_by(eid = anc['PC_UKBB.eid'])\n",
    "ancestry_to_remove = anc.filter(anc.group != \"United Kingdom\")\n",
    "print(f\"Ancestry to remove: {ancestry_to_remove.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f10af36f-dc12-4357-8530-7921987bb661",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-13 08:42:50.839 Hail: INFO: Reading table without type imputation       \n",
      "  Loading field 'f0' as type str (not specified)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Withdrawn samples to remove: 148\n"
     ]
    }
   ],
   "source": [
    "# Withdrawn samples to remove\n",
    "WITHDRAWN_FILE = \"file:///mnt/project/gogoGPCR2/withdrawn_2023-04-25.csv\"\n",
    "\n",
    "withdrawn_to_remove = hl.import_table(WITHDRAWN_FILE, delimiter = \",\", no_header = True).key_by(\"f0\")\n",
    "print(f\"Withdrawn samples to remove: {withdrawn_to_remove.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f4d6510-6c36-43b8-9fd6-094d6b29390d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-13 08:50:08.110 Hail: INFO: Reading table to impute column types        \n",
      "2023-09-13 08:50:09.220 Hail: INFO: Finished type imputation\n",
      "  Loading field 'ID1' as type str (user-supplied type)\n",
      "  Loading field 'ID2' as type str (user-supplied type)\n",
      "  Loading field 'HetHet' as type float64 (imputed)\n",
      "  Loading field 'IBS0' as type float64 (imputed)\n",
      "  Loading field 'Kinship' as type float64 (imputed)\n",
      "2023-09-13 08:50:14.794 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:50:17.096 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:50:20.314 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:50:22.316 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:50:24.999 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:50:26.448 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:50:33.030 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:50:34.215 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:50:36.471 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:50:37.062 Hail: INFO: Coerced sorted dataset\n",
      "2023-09-13 08:50:38.547 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:50:39.110 Hail: INFO: Coerced sorted dataset\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related samples not already in filter and high kinship coefficient: 27649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affective cases: 33439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-13 08:50:49.931 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:50:51.825 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:50:54.413 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:50:56.368 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:50:58.835 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:51:00.477 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:51:02.559 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:51:03.707 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:51:05.405 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:51:05.953 Hail: INFO: Coerced sorted dataset\n",
      "2023-09-13 08:51:07.469 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:51:08.107 Hail: INFO: Coerced sorted dataset\n",
      "2023-09-13 08:51:09.856 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:51:10.911 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:51:13.712 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:51:14.719 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:51:19.258 Hail: INFO: wrote table with 27649 rows in 1 partition to /tmp/jhLoQyVXJd3udmUQpXNpDh\n",
      "2023-09-13 08:51:27.216 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "[Stage 153:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples to remove to create independent set: 24848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Remove related individuals\n",
    "RAW_REL_FILE = Path(\"/mnt/project/Bulk/Genotype Results/Genotype calls/ukb_rel.dat\")\n",
    "MAX_KINSHIP = 0.176 # 2nd degree relatives\n",
    "\n",
    "\n",
    "rel = hl.import_table(\n",
    "    f\"file://{RAW_REL_FILE}\",\n",
    "    delimiter=\" \",\n",
    "    impute=True,\n",
    "    types={\"ID1\": \"str\", \"ID2\": \"str\"},\n",
    ")\n",
    "\n",
    "rel = rel.filter(\n",
    "    hl.is_defined(filtered_samples_to_remove[rel.ID1]) |\n",
    "    hl.is_defined(filtered_samples_to_remove[rel.ID2]) |\n",
    "    hl.is_defined(ancestry_to_remove[rel.ID1]) |\n",
    "    hl.is_defined(ancestry_to_remove[rel.ID2]) |\n",
    "    hl.is_defined(withdrawn_to_remove[rel.ID1]) |\n",
    "    hl.is_defined(withdrawn_to_remove[rel.ID2]),\n",
    "    keep = False\n",
    "    )\n",
    "\n",
    "rel = rel.filter(rel.Kinship > MAX_KINSHIP, keep=True)\n",
    "\n",
    "print(\n",
    "    f\"Related samples not already in filter and high kinship coefficient: {rel.count()}\"\n",
    ")\n",
    "\n",
    "# Define interesting cases for affective disorders tie breaker # TODO: Remove this shit \n",
    "fields = [\"20126\"]\n", 
    "\n",
    "field_names = [\n",
    "    fields_for_id(i, participant) for i in fields\n",
    "]\n",
    "\n",
    "field_names = [\"eid\"] + [field.name for fields in field_names for field in fields]\n",
    "\n",
    "df2 = participant.retrieve_fields(\n",
    "    names=field_names, engine=dxdata.connect(), coding_values=\"replace\"\n",
    ")\n",
    "\n",
    "# Add new_col based on old_col being null\n",
    "df2 = df2.withColumn(\"is_affective\", col(\"p20126_i0\").isNotNull() & (col(\"p20126_i0\") != \"No Bipolar or Depression\"))\n",
    "\n",
    "int_samples = hl.Table.from_spark(df2).key_by(\"eid\")\n",
    "\n",
    "print(f\"Affective cases: {int_samples.filter(int_samples.is_affective).count()}\")\n",
    "\n",
    "rel_interesting = rel.key_by(\n",
    "    i=hl.struct(id=rel.ID1,\n",
    "                is_affective=hl.or_else(int_samples[rel.ID1].is_affective, False)\n",
    "               ),\n",
    "    j=hl.struct(id=rel.ID2,\n",
    "                is_affective=hl.or_else(int_samples[rel.ID2].is_affective, False)\n",
    "               )\n",
    ")\n",
    "\n",
    "def tie_breaker(l, r):\n",
    "    return hl.if_else(l.is_affective & ~r.is_affective, -1, hl.if_else(~l.is_affective & r.is_affective, 1, 0))\n",
    "    \n",
    "related_samples_to_remove = hl.maximal_independent_set(\n",
    "        i=rel_interesting.i,\n",
    "        j=rel_interesting.j,\n",
    "        keep=False,\n",
    "        tie_breaker=tie_breaker\n",
    "    )\n",
    "\n",
    "related_samples_to_remove = related_samples_to_remove.key_by(eid = related_samples_to_remove.node.id).drop(\"node\")\n",
    "\n",
    "print(\n",
    "    f\"Samples to remove to create independent set: {related_samples_to_remove.count()}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae865841-797d-4be0-a376-a4dc2d7c1b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-13 08:55:41.864 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:55:48.920 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:55:49.544 Hail: INFO: Coerced sorted dataset\n",
      "2023-09-13 08:55:50.925 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 08:55:51.504 Hail: INFO: Coerced sorted dataset\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of samples to remove: 59423\n"
     ]
    }
   ],
   "source": [
    "final_to_remove = (\n",
    "    filtered_samples_to_remove\n",
    "    .join(related_samples_to_remove, how = \"outer\")\n",
    "    .join(ancestry_to_remove, how = \"outer\")\n",
    "    .join(withdrawn_to_remove, how = \"outer\")\n",
    ").distinct()\n",
    "\n",
    "print(f\"Final number of samples to remove: {final_to_remove.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "231d5a18-dbaa-480d-9add-dedef378842b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-13 09:22:30.867 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 09:22:37.555 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 09:22:38.145 Hail: INFO: Coerced sorted dataset\n",
      "2023-09-13 09:22:39.246 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2023-09-13 09:22:39.824 Hail: INFO: Coerced sorted dataset\n",
      "2023-09-13 09:22:44.635 Hail: INFO: merging 13 files totalling 464.2K...        \n",
      "2023-09-13 09:22:44.699 Hail: INFO: while writing:\n",
      "    /tmp/samples_to_remove.tsv\n",
      "  merge time: 63.099ms\n"
     ]
    }
   ],
   "source": [
    "SAMPLES_TO_REMOVE_FILE = \"/tmp/samples_to_remove.tsv\"\n",
    "\n",
    "final_to_remove.eid.export(SAMPLES_TO_REMOVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36529fbb-579e-4ab6-b799-a30a48a6f6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
